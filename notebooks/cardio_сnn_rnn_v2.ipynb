{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cardio_сnn_rnn_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCRvbEjoppx0",
        "outputId": "57e389cb-5d84-4495-a119-5aa0b3026a9f"
      },
      "source": [
        "#https://drive.google.com/file/d/1EVwP6MUQAeSnGM-ywBRHspCn_OWEfzJN/view?usp=sharing\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1EVwP6MUQAeSnGM-ywBRHspCn_OWEfzJN\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1EVwP6MUQAeSnGM-ywBRHspCn_OWEfzJN\" -o train.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    333      0 --:--:--  0:00:01 --:--:--   333\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100  955k  100  955k    0     0   592k      0  0:00:01  0:00:01 --:--:-- 10.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGCxbyAlM7Hc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "T_Xf4egHNAk-",
        "outputId": "18dd81fc-220e-48e1-98b0-076fe9c5c714"
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>800</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>780</td>\n",
              "      <td>780</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1572</td>\n",
              "      <td>792</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2392</td>\n",
              "      <td>820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>3196</td>\n",
              "      <td>804</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  time    x  y\n",
              "0   1     0  800  0\n",
              "1   1   780  780  0\n",
              "2   1  1572  792  0\n",
              "3   1  2392  820  0\n",
              "4   1  3196  804  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUdZ_3RoNiji",
        "outputId": "4f9a03dd-4597-410e-f1fb-bbe804b30ad8"
      },
      "source": [
        "df['marker'] = np.multiply(df[['id','y']].groupby('id').agg(np.cumsum).values.ravel(), df['y'].values)\n",
        "df['start'] = ((df['y'].shift(1, fill_value=0) == 0).values & (df['y'] == 1)).values\n",
        "df['end'] = ((df['y'].shift(-1, fill_value=0) == 0).values & (df['y'] == 1)).values\n",
        "\n",
        "q = df.loc[df['end'],'marker'] - df.loc[df['end'],'start']\n",
        "print('аномалий: ', q.shape[0], 'средняя длина: ', int(q.mean()*100.0)*0.01, 'min: ', q.min(), 'max: ', q.max())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "аномалий:  791 средняя длина:  25.060000000000002 min:  6 max:  81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "wH4pJnQwTsw4",
        "outputId": "d6ec60c2-14f7-4152-c6d6-41281f1b5b02"
      },
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "for q in [163]:\n",
        "    t = df.loc[df['id'] == q].sort_values('time').reset_index(drop=True)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=t['time'], y=t['x'] - t['x'].mean(),\n",
        "                        mode='lines',\n",
        "                        name='lines'))\n",
        "    qt = t.loc[t.y==1].reset_index(drop=True)\n",
        "    fig.add_trace(go.Scatter(x=qt['time'], y=qt['x'] - t['x'].mean(),\n",
        "                        mode='markers', name='markers'))\n",
        "    print(t.x.mean())\n",
        "    fig.show()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "728.0415584415584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d4cd3323-b123-410e-a6fa-940caea3f26f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d4cd3323-b123-410e-a6fa-940caea3f26f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd4cd3323-b123-410e-a6fa-940caea3f26f',\n",
              "                        [{\"mode\": \"lines\", \"name\": \"lines\", \"type\": \"scatter\", \"x\": [0, 400, 568, 1336, 1844, 1992, 2472, 3232, 4080, 4704, 5668, 5840, 6084, 6372, 6480, 7228, 8036, 8836, 9408, 9636, 9856, 10460, 11304, 12180, 13068, 13948, 14844, 15720, 16592, 17448, 18300, 19148, 19988, 20788, 21684, 22456, 23268, 24132, 24912, 25780, 26036, 26572, 27412, 28260, 29084, 29940, 30796, 31700, 32484, 33320, 34160, 35048, 35832, 36272, 36668, 37560, 38392, 39312, 39548, 40044, 40816, 41636, 42612, 42724, 43300, 44176, 44504, 44988, 45792, 46200, 46576, 47392, 48200, 48848, 49072, 49828, 50628, 51412, 52188, 52968, 53752, 54532, 55304, 56112, 56900, 57712, 58540, 59400, 60176, 61024, 61848, 62672, 63492, 64320, 65148, 65952, 66764, 67580, 68396, 69248, 70012, 70828, 71688, 72452, 73280, 74160, 74936, 75820, 76664, 77460, 78276, 79164, 79952, 80492, 80836, 81608, 82456, 83292, 84132, 84704, 84960, 85416, 85816, 86668, 87500, 88344, 89200, 90040, 90880, 91736, 92588, 93420, 94264, 95120, 95968, 96800, 97656, 98516, 99416, 100208, 101072, 101944, 102792, 103632, 104488, 105404, 106204, 107052, 107920, 108784, 109636, 110488, 111356, 112236, 113104, 113964, 114820, 115660, 116484, 117316, 118172, 119076, 119896, 120744, 121612, 122496, 123416, 124216, 125100, 125996, 126864, 127760, 128668, 129560, 130436, 131332, 132236, 133128, 134016, 134924, 135844, 136748, 137632, 138544, 139472, 140428, 141276, 142212, 143152, 144072, 145028, 145856, 146756, 147716, 148588, 149504, 150440, 151288, 152196, 153088, 154004, 154944, 155868, 156768, 157676, 158592, 159512, 160452, 161312, 162236, 163148, 164036, 164964, 165888, 166784, 167648, 168496, 169368, 170252, 171128, 172032, 172952, 173884, 174792, 175700, 176628, 177536, 178444, 179368, 180292, 181184, 182100, 183028, 183960, 184876, 185812, 186720, 187636, 188556, 189460, 190372, 191296, 192288, 193144, 194060, 195012, 195952, 196916, 197772, 198688, 199608, 200512, 201456, 202388, 203292, 204232, 205156, 206136, 206996, 207904, 208812, 209732, 210636, 211540, 212468, 213396, 214308, 215236, 216168, 217084, 218044, 218872, 219756, 220656, 221540, 222472, 223304, 224196, 225072, 225980, 227692, 228044, 228500, 229396, 230252, 231136, 232004, 232808, 232992, 233712, 234520, 234924, 235204, 235496, 236080, 236236, 236580, 236668, 237076, 237172, 237560, 237712, 237904, 238768, 239388, 239920, 240408, 241512, 242452, 242708, 242936, 243056, 243756, 244576, 245416, 246200, 247076, 247872, 248772, 249568, 250416, 251256, 252128, 252944, 253780, 254548, 255424, 256212, 257100, 257904, 258752, 259608, 260448, 261308, 262156, 263012, 263192, 263856, 264256, 264488, 264740, 265060, 265264, 265576, 265936, 266372, 267200, 268036, 268680, 268864, 269700, 269828, 270584, 271356, 271660, 273016, 273832, 274496, 274664, 275488, 275880, 276308, 277124, 277932, 278740, 279464, 279648, 280472, 280520, 280884, 281156, 281324, 281952, 282184, 282396, 282752, 283548, 284348, 285160, 285964, 286768, 287564, 288356, 289140, 289928, 290712, 291500, 292328, 293108, 293892, 294680, 295424, 296216, 297012, 297808, 298608, 299408, 300260, 301056, 301860, 302668, 303424, 304220, 305072, 305832, 306636, 307448, 308308, 309120, 309920, 310720, 311540, 312356, 313128, 313952, 314812, 315636, 316468, 317244, 318116, 318896, 319760, 320576, 321404, 322172, 322984, 323844, 324612, 325468, 326236, 327120, 327904, 328780, 329548, 330372, 331188, 332000, 332820, 333632, 334440, 335292, 335368, 336040, 336832, 337636, 338652, 338876, 339340, 339940, 340188, 341020, 341304, 341716, 341812, 342100, 342384, 342692, 343212, 343588, 344456, 345356, 346192, 347024, 347896, 348752, 349648, 349812, 349968, 350444, 351348, 352152, 353000, 354712, 355572, 356372, 357276, 358020, 358324, 358564, 358916, 359204, 359708, 360080, 360540, 360792, 360988, 361244, 362232, 362360, 363040, 363088, 363880, 364532, 364712, 364972, 365588, 366376, 367204, 368040, 368364, 368528, 368876, 369372, 369712, 370540, 371372, 372212, 373048, 373880, 374724, 375560, 376400, 377192, 377248, 377708, 377828, 378068, 378632, 378888, 379708, 380576, 381352, 382176, 383012, 383836, 384656, 385480, 386312, 387136, 387972, 388840, 389252, 389680, 390500, 390728, 391012, 391332, 392152, 392920, 393792, 394556, 394680, 394976, 395420, 396232, 396372, 397040, 397800, 398616, 399460, 400260, 401008, 401460, 401712, 402196, 402572, 403384, 403524, 403800, 404708, 404884, 405036, 405664, 407224, 408012, 408304, 408804, 409592, 410384, 411184, 411756, 411984, 413572, 414224, 414496, 415072, 415568, 415764, 416964, 417196, 417540, 418236, 419000, 419756, 420512, 421268, 422068, 422808, 423228, 423508, 424244, 424420, 424984, 425388, 425728, 426516, 427212, 427924, 428736, 429056, 429076, 429488, 429664, 429800, 430232, 430400, 430528, 430776, 431132, 431324, 431656, 431832, 432060, 432472, 433176, 433932, 434088, 434672, 435116, 435424, 436176, 436968, 437468, 437672, 438416, 439164, 439900, 439972, 440256, 440640, 440832, 441384, 442120, 442872, 443632, 444396, 445172, 445996, 446724, 447500, 448272, 448648, 448668, 449036, 449808, 450580, 451356, 452120, 452892, 453680, 454460, 455248, 456024, 456804, 457596, 458384, 459156, 459984, 460720, 461544, 462276, 462816, 463056, 463952, 464864, 465900, 466756, 467688, 468688, 469600, 469956, 470104, 470356, 470760, 471308, 471668, 472180, 472760, 473216, 474772, 475628, 475920, 475936, 476504, 477552, 477732, 477896, 478164, 478708, 478992, 479384, 479484, 479852, 480040, 480296, 480532, 480688, 481244, 481500, 482332, 482536, 482644, 483168, 483724, 484048, 484164, 484804, 485660, 485888, 486512, 486804, 486960, 487232, 487392, 488608, 488816, 490008, 490200, 490400, 490624, 490984, 491076, 491480, 492248, 492332, 493116, 493508, 493672, 493864, 494680, 494800, 495228, 495392, 495668, 496496, 496824, 498156, 498956, 499756, 500540, 501332, 502136, 502936, 503788, 504716, 505704, 506536, 507452, 508336, 509272, 510088, 510940, 511852, 512732, 513248, 513644, 514472, 515380, 516228, 517072, 517968, 518876, 519152, 519716, 520172, 520552, 521476, 522280, 523236, 523320, 523748, 524036, 524392, 524560, 524808, 525224, 525420, 525680, 525928, 526236, 526456, 526552, 527424, 528260, 528940, 529172, 529216, 529996, 530836, 531208, 531468, 531680, 532076, 532552, 533404, 534236, 535092, 535948, 536808, 537676, 538524, 539372, 540232, 541076, 541944, 542848, 543648, 544564, 545380, 546304, 547128, 547992, 548864, 549772, 550636, 551436, 552300, 553156, 554016, 554904, 555808, 556668, 557528, 558348, 559252, 560104, 560960, 561816, 562664, 563500, 564348, 565196, 566036, 566868, 567716, 568556, 569388, 570192, 571092, 571936, 572744, 573652, 574508, 576152, 577048, 577844, 578704, 579540, 580364, 581200, 581776, 582032, 582708, 582892, 583100, 583592, 584024, 584132, 584508, 584952, 585200, 585536, 585988, 586724, 586832, 587676, 588084, 588332, 588500, 589300, 590188, 590992, 591816, 592588, 593444, 594204, 595052, 596076, 596952, 597896, 598712, 599580, 599852, 599924, 600244, 600552, 601380, 601668, 602284, 603076, 603988, 604600, 604780, 605604, 606456, 607332, 608188, 609032, 609916, 610772, 611612, 612080, 612484, 613340, 614172, 615052, 615960, 616796, 617612, 618232, 618360, 618888, 619244, 619320, 621104, 621968, 622300, 622808, 623648, 623924, 624380, 624520, 625160, 625372, 625684, 625812, 626208, 626588, 626924, 627048, 627876, 628700, 629528, 630292, 631160, 631956, 632756, 633548, 634304, 635108, 635916, 636772, 637576, 638356, 639184, 640064, 640900, 641704, 642556, 643016, 643472, 644308, 645108, 645944, 646788, 647620, 648448, 649268, 650076, 650876, 651672, 652540, 653356, 654188, 654968, 655868, 656716, 657516, 658388, 659296, 660096, 660964, 661868, 662708, 663560, 664404, 665248, 666092, 666932, 667768, 668608, 669436, 670276, 671120, 671960, 672768, 673624, 674516, 675368, 676212, 677052, 677884, 678068, 678712, 678992, 679468, 680132, 680332, 681136, 681452, 681640, 681868, 682600, 682828, 683580, 684408, 685240, 686064, 686864, 687636, 688484, 689064, 689368, 690176, 691000, 691776, 692648, 693428, 694284, 694848, 695936, 696244, 696612, 696772, 697580, 698412, 699236, 700048, 700368, 700876, 701300, 701380, 701688, 702496, 703304, 704104, 704728, 704836, 705792, 706556, 707384, 708208, 709036, 709844, 710676, 711504, 712120, 712500, 713144, 713964, 714772, 715568, 716372, 717164, 717944, 718728, 719512, 720304, 721100, 721892, 722732, 723476, 724344, 725128, 725948, 726500, 726832, 727664, 728496, 729276, 729928, 730460, 730996, 771896, 772728, 773544, 774368, 775196, 776020, 776844, 777660, 778476, 779248, 780076, 780940, 781752, 782580, 783364, 784192, 785068, 785844, 786688, 787576, 788408, 789236, 790068, 790896, 791684, 792556, 793380, 794204, 794980, 795852, 796680, 797500, 798320, 799156, 800000, 800840, 801636, 802520, 803364, 804192, 805044, 805848, 806736, 807588, 808436, 809280, 810124, 810952, 811776, 812608, 813456, 814300, 815176, 816032, 816884, 817732, 818596, 819460, 820328, 821184, 822032, 822896, 823740, 824592, 825452, 826300, 827160, 828020, 828884, 829732, 830592, 831448, 832308, 833140, 834036, 834892, 835740, 836592, 837444, 838252, 839152, 839992], \"y\": [167.95844155844156, -328.04155844155844, -560.0415584415584, 39.958441558441564, -220.04155844155844, -580.0415584415584, -248.04155844155844, 31.958441558441564, 119.95844155844156, -104.04155844155844, 235.95844155844156, -556.0415584415584, -484.04155844155844, -440.04155844155844, -620.0415584415584, 19.958441558441564, 79.95844155844156, 71.95844155844156, -156.04155844155844, -500.04155844155844, -508.04155844155844, -124.04155844155844, 115.95844155844156, 147.95844155844156, 159.95844155844156, 151.95844155844156, 167.95844155844156, 147.95844155844156, 143.95844155844156, 127.95844155844156, 123.95844155844156, 119.95844155844156, 111.95844155844156, 71.95844155844156, 167.95844155844156, 43.958441558441564, 83.95844155844156, 135.95844155844156, 51.958441558441564, 139.95844155844156, -472.04155844155844, -192.04155844155844, 111.95844155844156, 119.95844155844156, 95.95844155844156, 127.95844155844156, 127.95844155844156, 175.95844155844156, 55.958441558441564, 107.95844155844156, 111.95844155844156, 159.95844155844156, 55.958441558441564, -288.04155844155844, -332.04155844155844, 163.95844155844156, 103.95844155844156, 191.95844155844156, -492.04155844155844, -232.04155844155844, 43.958441558441564, 91.95844155844156, 247.95844155844156, -616.0415584415584, -152.04155844155844, 147.95844155844156, -400.04155844155844, -244.04155844155844, 75.95844155844156, -320.04155844155844, -352.04155844155844, 87.95844155844156, 79.95844155844156, -80.04155844155844, -504.04155844155844, 27.958441558441564, 71.95844155844156, 55.958441558441564, 47.958441558441564, 51.958441558441564, 55.958441558441564, 51.958441558441564, 43.958441558441564, 79.95844155844156, 59.958441558441564, 83.95844155844156, 99.95844155844156, 131.95844155844156, 47.958441558441564, 119.95844155844156, 95.95844155844156, 95.95844155844156, 91.95844155844156, 99.95844155844156, 99.95844155844156, 75.95844155844156, 83.95844155844156, 87.95844155844156, 87.95844155844156, 123.95844155844156, 35.958441558441564, 87.95844155844156, 131.95844155844156, 35.958441558441564, 99.95844155844156, 151.95844155844156, 47.958441558441564, 155.95844155844156, 115.95844155844156, 67.95844155844156, 87.95844155844156, 159.95844155844156, 59.958441558441564, -188.04155844155844, -384.04155844155844, 43.958441558441564, 119.95844155844156, 107.95844155844156, 111.95844155844156, -156.04155844155844, -472.04155844155844, -272.04155844155844, -328.04155844155844, 123.95844155844156, 103.95844155844156, 115.95844155844156, 127.95844155844156, 111.95844155844156, 111.95844155844156, 127.95844155844156, 123.95844155844156, 103.95844155844156, 115.95844155844156, 127.95844155844156, 119.95844155844156, 103.95844155844156, 127.95844155844156, 131.95844155844156, 171.95844155844156, 63.958441558441564, 135.95844155844156, 143.95844155844156, 119.95844155844156, 111.95844155844156, 127.95844155844156, 187.95844155844156, 71.95844155844156, 119.95844155844156, 139.95844155844156, 135.95844155844156, 123.95844155844156, 123.95844155844156, 139.95844155844156, 151.95844155844156, 139.95844155844156, 131.95844155844156, 127.95844155844156, 111.95844155844156, 95.95844155844156, 103.95844155844156, 127.95844155844156, 175.95844155844156, 91.95844155844156, 119.95844155844156, 139.95844155844156, 155.95844155844156, 191.95844155844156, 71.95844155844156, 155.95844155844156, 167.95844155844156, 139.95844155844156, 167.95844155844156, 179.95844155844156, 163.95844155844156, 147.95844155844156, 167.95844155844156, 175.95844155844156, 163.95844155844156, 159.95844155844156, 179.95844155844156, 191.95844155844156, 175.95844155844156, 155.95844155844156, 183.95844155844156, 199.95844155844156, 227.95844155844156, 119.95844155844156, 207.95844155844156, 211.95844155844156, 191.95844155844156, 227.95844155844156, 99.95844155844156, 171.95844155844156, 231.95844155844156, 143.95844155844156, 187.95844155844156, 207.95844155844156, 119.95844155844156, 179.95844155844156, 163.95844155844156, 187.95844155844156, 211.95844155844156, 195.95844155844156, 171.95844155844156, 179.95844155844156, 187.95844155844156, 191.95844155844156, 211.95844155844156, 131.95844155844156, 195.95844155844156, 183.95844155844156, 159.95844155844156, 199.95844155844156, 195.95844155844156, 167.95844155844156, 135.95844155844156, 119.95844155844156, 143.95844155844156, 155.95844155844156, 147.95844155844156, 175.95844155844156, 191.95844155844156, 203.95844155844156, 179.95844155844156, 179.95844155844156, 199.95844155844156, 179.95844155844156, 179.95844155844156, 195.95844155844156, 195.95844155844156, 163.95844155844156, 187.95844155844156, 199.95844155844156, 203.95844155844156, 187.95844155844156, 207.95844155844156, 179.95844155844156, 187.95844155844156, 191.95844155844156, 175.95844155844156, 183.95844155844156, 195.95844155844156, 263.95844155844156, 127.95844155844156, 187.95844155844156, 223.95844155844156, 211.95844155844156, 235.95844155844156, 127.95844155844156, 187.95844155844156, 191.95844155844156, 175.95844155844156, 215.95844155844156, 203.95844155844156, 175.95844155844156, 211.95844155844156, 195.95844155844156, 251.95844155844156, 131.95844155844156, 179.95844155844156, 179.95844155844156, 191.95844155844156, 175.95844155844156, 175.95844155844156, 199.95844155844156, 199.95844155844156, 183.95844155844156, 199.95844155844156, 203.95844155844156, 187.95844155844156, 231.95844155844156, 99.95844155844156, 155.95844155844156, 171.95844155844156, 155.95844155844156, 203.95844155844156, 103.95844155844156, 163.95844155844156, 147.95844155844156, 179.95844155844156, 983.9584415584416, -376.04155844155844, -272.04155844155844, 167.95844155844156, 127.95844155844156, 155.95844155844156, 139.95844155844156, 75.95844155844156, -544.0415584415584, -8.041558441558436, 79.95844155844156, -324.04155844155844, -448.04155844155844, -436.04155844155844, -144.04155844155844, -572.0415584415584, -384.04155844155844, -640.0415584415584, -320.04155844155844, -632.0415584415584, -340.04155844155844, -576.0415584415584, -536.0415584415584, 135.95844155844156, -108.04155844155844, -196.04155844155844, -240.04155844155844, 375.95844155844156, 211.95844155844156, -472.04155844155844, -500.04155844155844, -608.0415584415584, -28.041558441558436, 91.95844155844156, 111.95844155844156, 55.958441558441564, 147.95844155844156, 67.95844155844156, 171.95844155844156, 67.95844155844156, 119.95844155844156, 111.95844155844156, 143.95844155844156, 87.95844155844156, 107.95844155844156, 39.958441558441564, 147.95844155844156, 59.958441558441564, 159.95844155844156, 75.95844155844156, 119.95844155844156, 127.95844155844156, 111.95844155844156, 131.95844155844156, 119.95844155844156, 127.95844155844156, -548.0415584415584, -64.04155844155844, -328.04155844155844, -496.04155844155844, -476.04155844155844, -408.04155844155844, -524.0415584415584, -416.04155844155844, -368.04155844155844, -292.04155844155844, 99.95844155844156, 107.95844155844156, -84.04155844155844, -544.0415584415584, 107.95844155844156, -600.0415584415584, 27.958441558441564, 43.958441558441564, -424.04155844155844, 627.9584415584416, 87.95844155844156, -64.04155844155844, -560.0415584415584, 95.95844155844156, -336.04155844155844, -300.04155844155844, 87.95844155844156, 79.95844155844156, 79.95844155844156, -4.041558441558436, -544.0415584415584, 95.95844155844156, -680.0415584415584, -364.04155844155844, -456.04155844155844, -560.0415584415584, -100.04155844155844, -496.04155844155844, -516.0415584415584, -372.04155844155844, 67.95844155844156, 71.95844155844156, 83.95844155844156, 75.95844155844156, 75.95844155844156, 67.95844155844156, 63.958441558441564, 55.958441558441564, 59.958441558441564, 55.958441558441564, 59.958441558441564, 99.95844155844156, 51.958441558441564, 55.958441558441564, 59.958441558441564, 15.958441558441564, 63.958441558441564, 67.95844155844156, 67.95844155844156, 71.95844155844156, 71.95844155844156, 123.95844155844156, 67.95844155844156, 75.95844155844156, 79.95844155844156, 27.958441558441564, 67.95844155844156, 123.95844155844156, 31.958441558441564, 75.95844155844156, 83.95844155844156, 131.95844155844156, 83.95844155844156, 71.95844155844156, 71.95844155844156, 91.95844155844156, 87.95844155844156, 43.958441558441564, 95.95844155844156, 131.95844155844156, 95.95844155844156, 103.95844155844156, 47.958441558441564, 143.95844155844156, 51.958441558441564, 135.95844155844156, 87.95844155844156, 99.95844155844156, 39.958441558441564, 83.95844155844156, 131.95844155844156, 39.958441558441564, 127.95844155844156, 39.958441558441564, 155.95844155844156, 55.958441558441564, 147.95844155844156, 39.958441558441564, 95.95844155844156, 87.95844155844156, 83.95844155844156, 91.95844155844156, 83.95844155844156, 79.95844155844156, 123.95844155844156, -652.0415584415584, -56.041558441558436, 63.958441558441564, 75.95844155844156, 287.95844155844156, -504.04155844155844, -264.04155844155844, -128.04155844155844, -480.04155844155844, 103.95844155844156, -444.04155844155844, -316.04155844155844, -632.0415584415584, -440.04155844155844, -444.04155844155844, -420.04155844155844, -208.04155844155844, -352.04155844155844, 139.95844155844156, 171.95844155844156, 107.95844155844156, 103.95844155844156, 143.95844155844156, 127.95844155844156, 167.95844155844156, -564.0415584415584, -572.0415584415584, -252.04155844155844, 175.95844155844156, 75.95844155844156, 119.95844155844156, 983.9584415584416, 131.95844155844156, 71.95844155844156, 175.95844155844156, 15.958441558441564, -424.04155844155844, -488.04155844155844, -376.04155844155844, -440.04155844155844, -224.04155844155844, -356.04155844155844, -268.04155844155844, -476.04155844155844, -532.0415584415584, -472.04155844155844, 259.95844155844156, -600.0415584415584, -48.041558441558436, -680.0415584415584, 63.958441558441564, -76.04155844155844, -548.0415584415584, -468.04155844155844, -112.04155844155844, 59.958441558441564, 99.95844155844156, 107.95844155844156, -404.04155844155844, -564.0415584415584, -380.04155844155844, -232.04155844155844, -388.04155844155844, 99.95844155844156, 103.95844155844156, 111.95844155844156, 107.95844155844156, 103.95844155844156, 115.95844155844156, 107.95844155844156, 111.95844155844156, 63.958441558441564, -672.0415584415584, -268.04155844155844, -608.0415584415584, -488.04155844155844, -164.04155844155844, -472.04155844155844, 91.95844155844156, 139.95844155844156, 47.958441558441564, 95.95844155844156, 107.95844155844156, 95.95844155844156, 91.95844155844156, 95.95844155844156, 103.95844155844156, 95.95844155844156, 107.95844155844156, 139.95844155844156, -316.04155844155844, -300.04155844155844, 91.95844155844156, -500.04155844155844, -444.04155844155844, -408.04155844155844, 91.95844155844156, 39.958441558441564, 143.95844155844156, 35.958441558441564, -604.0415584415584, -432.04155844155844, -284.04155844155844, 83.95844155844156, -588.0415584415584, -60.041558441558436, 31.958441558441564, 87.95844155844156, 115.95844155844156, 71.95844155844156, 19.958441558441564, -276.04155844155844, -476.04155844155844, -244.04155844155844, -352.04155844155844, 83.95844155844156, -588.0415584415584, -452.04155844155844, 179.95844155844156, -552.0415584415584, -576.0415584415584, -100.04155844155844, 831.9584415584416, 59.958441558441564, -436.04155844155844, -228.04155844155844, 59.958441558441564, 63.958441558441564, 71.95844155844156, -156.04155844155844, -500.04155844155844, 859.9584415584416, -76.04155844155844, -456.04155844155844, -152.04155844155844, -232.04155844155844, -532.0415584415584, 471.95844155844156, -496.04155844155844, -384.04155844155844, -32.041558441558436, 35.958441558441564, 27.958441558441564, 27.958441558441564, 27.958441558441564, 71.95844155844156, 11.958441558441564, -308.04155844155844, -448.04155844155844, 7.958441558441564, -552.0415584415584, -164.04155844155844, -324.04155844155844, -388.04155844155844, 59.958441558441564, -32.041558441558436, -16.041558441558436, 83.95844155844156, -408.04155844155844, -708.0415584415584, -316.04155844155844, -552.0415584415584, -592.0415584415584, -296.04155844155844, -560.0415584415584, -600.0415584415584, -480.04155844155844, -372.04155844155844, -536.0415584415584, -396.04155844155844, -552.0415584415584, -500.04155844155844, -316.04155844155844, -24.041558441558436, 27.958441558441564, -572.0415584415584, -144.04155844155844, -284.04155844155844, -420.04155844155844, 23.958441558441564, 63.958441558441564, -228.04155844155844, -524.0415584415584, 15.958441558441564, 19.958441558441564, 7.958441558441564, -656.0415584415584, -444.04155844155844, -344.04155844155844, -536.0415584415584, -176.04155844155844, 7.958441558441564, 23.958441558441564, 31.958441558441564, 35.958441558441564, 47.958441558441564, 95.95844155844156, -0.04155844155843624, 47.958441558441564, 43.958441558441564, -352.04155844155844, -708.0415584415584, -360.04155844155844, 43.958441558441564, 43.958441558441564, 47.958441558441564, 35.958441558441564, 43.958441558441564, 59.958441558441564, 51.958441558441564, 59.958441558441564, 47.958441558441564, 51.958441558441564, 63.958441558441564, 59.958441558441564, 43.958441558441564, 99.95844155844156, 7.958441558441564, 95.95844155844156, 3.9584415584415638, -188.04155844155844, -488.04155844155844, 167.95844155844156, 183.95844155844156, 307.95844155844156, 127.95844155844156, 203.95844155844156, 271.95844155844156, 183.95844155844156, -372.04155844155844, -580.0415584415584, -476.04155844155844, -324.04155844155844, -180.04155844155844, -368.04155844155844, -216.04155844155844, -148.04155844155844, -272.04155844155844, 827.9584415584416, 127.95844155844156, -436.04155844155844, -712.0415584415584, -160.04155844155844, 319.95844155844156, -548.0415584415584, -564.0415584415584, -460.04155844155844, -184.04155844155844, -444.04155844155844, -336.04155844155844, -628.0415584415584, -360.04155844155844, -540.0415584415584, -472.04155844155844, -492.04155844155844, -572.0415584415584, -172.04155844155844, -472.04155844155844, 103.95844155844156, -524.0415584415584, -620.0415584415584, -204.04155844155844, -172.04155844155844, -404.04155844155844, -612.0415584415584, -88.04155844155844, 127.95844155844156, -500.04155844155844, -104.04155844155844, -436.04155844155844, -572.0415584415584, -456.04155844155844, -568.0415584415584, 487.95844155844156, -520.0415584415584, 463.95844155844156, -536.0415584415584, -528.0415584415584, -504.04155844155844, -368.04155844155844, -636.0415584415584, -324.04155844155844, 39.958441558441564, -644.0415584415584, 55.958441558441564, -336.04155844155844, -564.0415584415584, -536.0415584415584, 87.95844155844156, -608.0415584415584, -300.04155844155844, -564.0415584415584, -452.04155844155844, 99.95844155844156, -400.04155844155844, 603.9584415584416, 71.95844155844156, 71.95844155844156, 55.958441558441564, 63.958441558441564, 75.95844155844156, 71.95844155844156, 123.95844155844156, 199.95844155844156, 259.95844155844156, 103.95844155844156, 187.95844155844156, 155.95844155844156, 207.95844155844156, 87.95844155844156, 123.95844155844156, 183.95844155844156, 151.95844155844156, -212.04155844155844, -332.04155844155844, 99.95844155844156, 179.95844155844156, 119.95844155844156, 115.95844155844156, 167.95844155844156, 179.95844155844156, -452.04155844155844, -164.04155844155844, -272.04155844155844, -348.04155844155844, 195.95844155844156, 75.95844155844156, 227.95844155844156, -644.0415584415584, -300.04155844155844, -440.04155844155844, -372.04155844155844, -560.0415584415584, -480.04155844155844, -312.04155844155844, -532.0415584415584, -468.04155844155844, -480.04155844155844, -420.04155844155844, -508.04155844155844, -632.0415584415584, 143.95844155844156, 107.95844155844156, -48.041558441558436, -496.04155844155844, -684.0415584415584, 51.958441558441564, 111.95844155844156, -356.04155844155844, -468.04155844155844, -516.0415584415584, -332.04155844155844, -252.04155844155844, 123.95844155844156, 103.95844155844156, 127.95844155844156, 127.95844155844156, 131.95844155844156, 139.95844155844156, 119.95844155844156, 119.95844155844156, 131.95844155844156, 115.95844155844156, 139.95844155844156, 175.95844155844156, 71.95844155844156, 187.95844155844156, 87.95844155844156, 195.95844155844156, 95.95844155844156, 135.95844155844156, 143.95844155844156, 179.95844155844156, 135.95844155844156, 71.95844155844156, 135.95844155844156, 127.95844155844156, 131.95844155844156, 159.95844155844156, 175.95844155844156, 131.95844155844156, 131.95844155844156, 91.95844155844156, 175.95844155844156, 123.95844155844156, 127.95844155844156, 127.95844155844156, 119.95844155844156, 107.95844155844156, 119.95844155844156, 119.95844155844156, 111.95844155844156, 103.95844155844156, 119.95844155844156, 111.95844155844156, 103.95844155844156, 75.95844155844156, 171.95844155844156, 115.95844155844156, 79.95844155844156, 179.95844155844156, 127.95844155844156, 915.9584415584416, 167.95844155844156, 67.95844155844156, 131.95844155844156, 107.95844155844156, 95.95844155844156, 107.95844155844156, -152.04155844155844, -472.04155844155844, -52.041558441558436, -544.0415584415584, -520.0415584415584, -236.04155844155844, -296.04155844155844, -620.0415584415584, -352.04155844155844, -284.04155844155844, -480.04155844155844, -392.04155844155844, -276.04155844155844, 7.958441558441564, -620.0415584415584, 115.95844155844156, -320.04155844155844, -480.04155844155844, -560.0415584415584, 71.95844155844156, 159.95844155844156, 75.95844155844156, 95.95844155844156, 43.958441558441564, 127.95844155844156, 31.958441558441564, 119.95844155844156, 295.95844155844156, 147.95844155844156, 215.95844155844156, 87.95844155844156, 139.95844155844156, -456.04155844155844, -656.0415584415584, -408.04155844155844, -420.04155844155844, 99.95844155844156, -440.04155844155844, -112.04155844155844, 63.958441558441564, 183.95844155844156, -116.04155844155844, -548.0415584415584, 95.95844155844156, 123.95844155844156, 147.95844155844156, 127.95844155844156, 115.95844155844156, 155.95844155844156, 127.95844155844156, 111.95844155844156, -260.04155844155844, -324.04155844155844, 127.95844155844156, 103.95844155844156, 151.95844155844156, 179.95844155844156, 107.95844155844156, 87.95844155844156, -108.04155844155844, -600.0415584415584, -200.04155844155844, -372.04155844155844, -652.0415584415584, 1055.9584415584416, 135.95844155844156, -396.04155844155844, -220.04155844155844, 111.95844155844156, -452.04155844155844, -272.04155844155844, -588.0415584415584, -88.04155844155844, -516.0415584415584, -416.04155844155844, -600.0415584415584, -332.04155844155844, -348.04155844155844, -392.04155844155844, -604.0415584415584, 99.95844155844156, 95.95844155844156, 99.95844155844156, 35.958441558441564, 139.95844155844156, 67.95844155844156, 71.95844155844156, 63.958441558441564, 27.958441558441564, 75.95844155844156, 79.95844155844156, 127.95844155844156, 75.95844155844156, 51.958441558441564, 99.95844155844156, 151.95844155844156, 107.95844155844156, 75.95844155844156, 123.95844155844156, -268.04155844155844, -272.04155844155844, 107.95844155844156, 71.95844155844156, 107.95844155844156, 115.95844155844156, 103.95844155844156, 99.95844155844156, 91.95844155844156, 79.95844155844156, 71.95844155844156, 67.95844155844156, 139.95844155844156, 87.95844155844156, 103.95844155844156, 51.958441558441564, 171.95844155844156, 119.95844155844156, 71.95844155844156, 143.95844155844156, 179.95844155844156, 71.95844155844156, 139.95844155844156, 175.95844155844156, 111.95844155844156, 123.95844155844156, 115.95844155844156, 115.95844155844156, 115.95844155844156, 111.95844155844156, 107.95844155844156, 111.95844155844156, 99.95844155844156, 111.95844155844156, 115.95844155844156, 111.95844155844156, 79.95844155844156, 127.95844155844156, 163.95844155844156, 123.95844155844156, 115.95844155844156, 111.95844155844156, 103.95844155844156, -544.0415584415584, -84.04155844155844, -448.04155844155844, -252.04155844155844, -64.04155844155844, -528.0415584415584, 75.95844155844156, -412.04155844155844, -540.0415584415584, -500.04155844155844, 3.9584415584415638, -500.04155844155844, 23.958441558441564, 99.95844155844156, 103.95844155844156, 95.95844155844156, 71.95844155844156, 43.958441558441564, 119.95844155844156, -148.04155844155844, -424.04155844155844, 79.95844155844156, 95.95844155844156, 47.958441558441564, 143.95844155844156, 51.958441558441564, 127.95844155844156, -164.04155844155844, 359.95844155844156, -420.04155844155844, -360.04155844155844, -568.0415584415584, 79.95844155844156, 103.95844155844156, 95.95844155844156, 83.95844155844156, -408.04155844155844, -220.04155844155844, -304.04155844155844, -648.0415584415584, -420.04155844155844, 79.95844155844156, 79.95844155844156, 71.95844155844156, -104.04155844155844, -620.0415584415584, 227.95844155844156, 35.958441558441564, 99.95844155844156, 95.95844155844156, 99.95844155844156, 79.95844155844156, 103.95844155844156, 99.95844155844156, -112.04155844155844, -348.04155844155844, -84.04155844155844, 91.95844155844156, 79.95844155844156, 67.95844155844156, 75.95844155844156, 63.958441558441564, 51.958441558441564, 55.958441558441564, 55.958441558441564, 63.958441558441564, 67.95844155844156, 63.958441558441564, 111.95844155844156, 15.958441558441564, 139.95844155844156, 55.958441558441564, 91.95844155844156, -176.04155844155844, -396.04155844155844, 103.95844155844156, 103.95844155844156, 51.958441558441564, -76.04155844155844, -196.04155844155844, -192.04155844155844, 40171.95844155844, 103.95844155844156, 87.95844155844156, 95.95844155844156, 99.95844155844156, 95.95844155844156, 95.95844155844156, 87.95844155844156, 87.95844155844156, 43.958441558441564, 99.95844155844156, 135.95844155844156, 83.95844155844156, 99.95844155844156, 55.958441558441564, 99.95844155844156, 147.95844155844156, 47.958441558441564, 115.95844155844156, 159.95844155844156, 103.95844155844156, 99.95844155844156, 103.95844155844156, 99.95844155844156, 59.958441558441564, 143.95844155844156, 95.95844155844156, 95.95844155844156, 47.958441558441564, 143.95844155844156, 99.95844155844156, 91.95844155844156, 91.95844155844156, 107.95844155844156, 115.95844155844156, 111.95844155844156, 67.95844155844156, 155.95844155844156, 115.95844155844156, 99.95844155844156, 123.95844155844156, 75.95844155844156, 159.95844155844156, 123.95844155844156, 119.95844155844156, 115.95844155844156, 115.95844155844156, 99.95844155844156, 95.95844155844156, 103.95844155844156, 119.95844155844156, 115.95844155844156, 147.95844155844156, 127.95844155844156, 123.95844155844156, 119.95844155844156, 135.95844155844156, 135.95844155844156, 139.95844155844156, 127.95844155844156, 119.95844155844156, 135.95844155844156, 115.95844155844156, 123.95844155844156, 131.95844155844156, 119.95844155844156, 131.95844155844156, 131.95844155844156, 135.95844155844156, 119.95844155844156, 131.95844155844156, 127.95844155844156, 131.95844155844156, 103.95844155844156, 167.95844155844156, 127.95844155844156, 119.95844155844156, 123.95844155844156, 123.95844155844156, 79.95844155844156, 171.95844155844156, 111.95844155844156]}, {\"mode\": \"markers\", \"name\": \"markers\", \"type\": \"scatter\", \"x\": [95968, 96800, 97656, 98516, 99416, 100208, 101072, 101944, 102792, 103632, 104488, 105404, 106204, 107052, 107920, 108784, 109636, 187636, 188556, 189460, 190372, 191296, 192288, 193144, 194060, 195012, 195952, 203292, 204232, 205156, 206136, 206996, 207904, 208812, 209732, 210636, 211540, 212468, 214308, 215236, 216168, 217084, 218044, 218872, 219756, 220656, 221540, 222472, 223304, 224196, 225072], \"y\": [119.95844155844156, 103.95844155844156, 127.95844155844156, 131.95844155844156, 171.95844155844156, 63.958441558441564, 135.95844155844156, 143.95844155844156, 119.95844155844156, 111.95844155844156, 127.95844155844156, 187.95844155844156, 71.95844155844156, 119.95844155844156, 139.95844155844156, 135.95844155844156, 123.95844155844156, 187.95844155844156, 191.95844155844156, 175.95844155844156, 183.95844155844156, 195.95844155844156, 263.95844155844156, 127.95844155844156, 187.95844155844156, 223.95844155844156, 211.95844155844156, 175.95844155844156, 211.95844155844156, 195.95844155844156, 251.95844155844156, 131.95844155844156, 179.95844155844156, 179.95844155844156, 191.95844155844156, 175.95844155844156, 175.95844155844156, 199.95844155844156, 183.95844155844156, 199.95844155844156, 203.95844155844156, 187.95844155844156, 231.95844155844156, 99.95844155844156, 155.95844155844156, 171.95844155844156, 155.95844155844156, 203.95844155844156, 103.95844155844156, 163.95844155844156, 147.95844155844156]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d4cd3323-b123-410e-a6fa-940caea3f26f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ahkhIR1Z5c9"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def scale_ts(vls):\n",
        "    sc = StandardScaler()\n",
        "    return sc.fit_transform(vls.reshape(-1,1)).ravel()\n",
        "def scale_time_ts(vls):\n",
        "    sc = MinMaxScaler()\n",
        "    return sc.fit_transform(vls.reshape(-1,1)).ravel()\n",
        "\n",
        "class CardioDataset(Dataset):\n",
        "    def __init__(self, df, win_size=32):\n",
        "        self.df = df.sort_values(['id','time']).reset_index(drop=True).copy()\n",
        "        self.df['time'] = df.groupby('id')['time'].agg('diff').fillna(0).values\n",
        "        self.df['time'] = scale_time_ts(self.df['time'].values)\n",
        "        self.win_size = win_size\n",
        "\n",
        "        self.point_indexes = []\n",
        "        self.win_lens = []\n",
        "        dfs = []\n",
        "        total_len = 0\n",
        "        for q,qdf in self.df.groupby('id'):\n",
        "            #qdf['x'] = scale_ts(qdf['x'].values)\n",
        "            for i in range(max(1, qdf.shape[0] - win_size + 1)):\n",
        "                self.point_indexes.append(i + total_len)\n",
        "                if i + win_size > qdf.shape[0]:\n",
        "                    self.win_lens.append(qdf.shape[0] - i)\n",
        "                else:\n",
        "                    self.win_lens.append(win_size)\n",
        "            total_len += qdf.shape[0]\n",
        "            dfs.append(qdf)\n",
        "        self.df = pd.concat(dfs, ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.point_indexes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i0 = self.point_indexes[idx]\n",
        "        i1 = i0 + self.win_lens[idx]\n",
        "\n",
        "        x_mat = np.zeros((self.win_size,2))\n",
        "        y_mat = np.zeros(self.win_size)\n",
        "        x_mat[-self.win_lens[idx]:,0] = scale_ts(self.df.iloc[i0:i1].x.values)\n",
        "        x_mat[-self.win_lens[idx]:,1] = self.df.iloc[i0:i1].time.values\n",
        "        y_mat[-self.win_lens[idx]:] = self.df.iloc[i0:i1].y.values\n",
        "        \n",
        "        return {\"x\": x_mat,\n",
        "                \"y\": y_mat,\n",
        "                \"start\": i0,\n",
        "                \"end\": i1\n",
        "               }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsEg7OS_fzsV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "class CardioRnn(nn.Module):\n",
        "    def __init__(self, win_size, output_size, rnn_units = 32):\n",
        "        super().__init__()\n",
        "        self._gru = nn.GRU(input_size=2, \n",
        "                           num_layers=4,\n",
        "                           dropout=0.3,\n",
        "                           hidden_size=rnn_units, \n",
        "                           batch_first=True, \n",
        "                           bidirectional=True)\n",
        "\n",
        "        self._head = nn.Linear(in_features = 4 * rnn_units, \n",
        "                               out_features=output_size)\n",
        "        \n",
        "        self.fcdrop2 = nn.Dropout(p=0.3)\n",
        "        \n",
        "    def forward(self, x_feats):\n",
        "        encoded, _ = self._gru(x_feats)\n",
        "        x2 = torch.max(encoded, axis=1).values\n",
        "        x2 = torch.squeeze(x2)\n",
        "\n",
        "        x3 = torch.mean(encoded, axis=1)\n",
        "        x3 = torch.squeeze(x3)\n",
        "\n",
        "        x = torch.cat([x2,x3],-1)\n",
        "        x = F.relu(x)\n",
        "        x = self.fcdrop2(x)\n",
        "        x = self._head(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51dYrZWih7yb",
        "outputId": "838900f5-c0f2-4e7a-e1a5-fe2bad84486b"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "model = CardioRnn(32, 32).to(device)\n",
        "\n",
        "ds = CardioDataset(df, 32)\n",
        "print(len(ds))\n",
        "train_sampler = SequentialSampler(ds)\n",
        "batch_size = 1\n",
        "train_dl = DataLoader(ds, sampler=train_sampler, batch_size=batch_size, num_workers=2)\n",
        "for x in train_dl:\n",
        "    print(model(x['x'].float().to(device)))\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53388\n",
            "tensor([ 0.0985, -0.0467, -0.1338,  0.0092,  0.1249,  0.0026,  0.0857,  0.2297,\n",
            "         0.0979, -0.0049,  0.0813,  0.0746, -0.0838,  0.1438,  0.0042,  0.0101,\n",
            "        -0.1735,  0.0255, -0.0758, -0.1046,  0.1656,  0.0018, -0.0560,  0.0851,\n",
            "         0.0245,  0.0890,  0.0751,  0.0363,  0.1544, -0.1678,  0.1092, -0.0587],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vACws6kfmzlF"
      },
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n",
        "\n",
        "def threshold_search(y_true, y_proba):\n",
        "    precision , recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
        "    thresholds = np.append(thresholds, 1.001) \n",
        "    F = 2 / (1/precision + 1/recall)\n",
        "    best_score = np.max(F)\n",
        "    best_th = thresholds[np.argmax(F)]\n",
        "    return best_th , best_score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep0hOJh5o1Ph",
        "outputId": "78cef301-1d48-4293-d4f6-91a0fbef30d0"
      },
      "source": [
        "%%writefile lookahead.py\n",
        "# Lookahead implementation from https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/lookahead.py\n",
        "\n",
        "\"\"\" Lookahead Optimizer Wrapper.\n",
        "Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
        "Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from collections import defaultdict\n",
        "\n",
        "class Lookahead(Optimizer):\n",
        "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
        "        self.base_optimizer = base_optimizer\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults = base_optimizer.defaults\n",
        "        self.defaults.update(defaults)\n",
        "        self.state = defaultdict(dict)\n",
        "        # manually add our defaults to the param groups\n",
        "        for name, default in defaults.items():\n",
        "            for group in self.param_groups:\n",
        "                group.setdefault(name, default)\n",
        "\n",
        "    def update_slow(self, group):\n",
        "        for fast_p in group[\"params\"]:\n",
        "            if fast_p.grad is None:\n",
        "                continue\n",
        "            param_state = self.state[fast_p]\n",
        "            if 'slow_buffer' not in param_state:\n",
        "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
        "                param_state['slow_buffer'].copy_(fast_p.data)\n",
        "            slow = param_state['slow_buffer']\n",
        "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
        "            fast_p.data.copy_(slow)\n",
        "\n",
        "    def sync_lookahead(self):\n",
        "        for group in self.param_groups:\n",
        "            self.update_slow(group)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # print(self.k)\n",
        "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
        "        loss = self.base_optimizer.step(closure)\n",
        "        for group in self.param_groups:\n",
        "            group['lookahead_step'] += 1\n",
        "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
        "                self.update_slow(group)\n",
        "        return loss\n",
        "\n",
        "    def state_dict(self):\n",
        "        fast_state_dict = self.base_optimizer.state_dict()\n",
        "        slow_state = {\n",
        "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
        "            for k, v in self.state.items()\n",
        "        }\n",
        "        fast_state = fast_state_dict['state']\n",
        "        param_groups = fast_state_dict['param_groups']\n",
        "        return {\n",
        "            'state': fast_state,\n",
        "            'slow_state': slow_state,\n",
        "            'param_groups': param_groups,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        fast_state_dict = {\n",
        "            'state': state_dict['state'],\n",
        "            'param_groups': state_dict['param_groups'],\n",
        "        }\n",
        "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
        "\n",
        "        # We want to restore the slow state, but share param_groups reference\n",
        "        # with base_optimizer. This is a bit redundant but least code\n",
        "        slow_state_new = False\n",
        "        if 'slow_state' not in state_dict:\n",
        "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
        "            state_dict['slow_state'] = defaultdict(dict)\n",
        "            slow_state_new = True\n",
        "        slow_state_dict = {\n",
        "            'state': state_dict['slow_state'],\n",
        "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
        "        }\n",
        "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
        "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
        "        if slow_state_new:\n",
        "            # reapply defaults to catch missing lookahead specific ones\n",
        "            for name, default in self.defaults.items():\n",
        "                for group in self.param_groups:\n",
        "                    group.setdefault(name, default)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing lookahead.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvnXUf0To6Lu",
        "outputId": "f40e6202-4966-4542-c768-6f1585a2c1a4"
      },
      "source": [
        "%%writefile ralamb.py\n",
        "import torch, math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "# RAdam + LARS\n",
        "class Ralamb(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(Ralamb, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Ralamb, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = radam_step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                radam_step = p_data_fp32.clone()\n",
        "                if N_sma >= 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
        "\n",
        "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                if weight_norm == 0 or radam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / radam_norm\n",
        "\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = radam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "\n",
        "                if N_sma >= 5:\n",
        "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ralamb.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkeKVWauo8az"
      },
      "source": [
        "from lookahead import *\n",
        "from ralamb import * \n",
        "\n",
        "def Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n",
        "     ralamb = Ralamb(params, *args, **kwargs)\n",
        "     return Lookahead(ralamb, alpha, k)\n",
        "\n",
        "RangerLars = Over9000"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNHDupvdxJJ6"
      },
      "source": [
        "class CardioCnn(nn.Module):\n",
        "    def __init__(self, win_size, output_size, \n",
        "                 out_chan = 64, \n",
        "                 top_classifier_units = 64,\n",
        "                 rnn_units = 8,\n",
        "                 kern_sizes = [3,5,7,9]):\n",
        "        super().__init__()\n",
        "        self.cnn0 = nn.Conv1d(2, out_chan, kern_sizes[0])\n",
        "        self.cnn1 = nn.Conv1d(2, out_chan, kern_sizes[1])\n",
        "        self.cnn2 = nn.Conv1d(2, out_chan, kern_sizes[2])\n",
        "        self.cnn3 = nn.Conv1d(2, out_chan, kern_sizes[3])\n",
        "\n",
        "        self._gru = nn.GRU(input_size=2, \n",
        "                           num_layers=4,\n",
        "                           dropout=0.3,\n",
        "                           hidden_size=rnn_units, \n",
        "                           batch_first=True, \n",
        "                           bidirectional=True)\n",
        "\n",
        "        self.sum_chans = (win_size*4 - np.sum(kern_sizes) + 4) + 2*rnn_units\n",
        "\n",
        "        self._head = nn.Sequential(\n",
        "            nn.LayerNorm(self.sum_chans),\n",
        "            nn.Linear(self.sum_chans, top_classifier_units),\n",
        "            nn.LayerNorm(top_classifier_units),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(top_classifier_units, top_classifier_units),\n",
        "            nn.LayerNorm(top_classifier_units),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(top_classifier_units, output_size),            \n",
        "        ) \n",
        "        \n",
        "    def forward(self, x_feats):\n",
        "        encoded,_ = self._gru(x_feats)\n",
        "        encoded0 = self.cnn0(x_feats.swapaxes(1,2))\n",
        "        encoded1 = self.cnn1(x_feats.swapaxes(1,2))\n",
        "        encoded2 = self.cnn2(x_feats.swapaxes(1,2))\n",
        "        encoded3 = self.cnn3(x_feats.swapaxes(1,2))\n",
        "\n",
        "        x0 = torch.max(encoded0, axis=1).values\n",
        "        x0 = torch.squeeze(x0)\n",
        "        x1 = torch.mean(encoded0, axis=1)\n",
        "        x1 = torch.squeeze(x1)\n",
        "\n",
        "        x2 = torch.max(encoded1, axis=1).values\n",
        "        x2 = torch.squeeze(x2)\n",
        "        x3 = torch.mean(encoded1, axis=1)\n",
        "        x3 = torch.squeeze(x3)\n",
        "\n",
        "        x4 = torch.max(encoded2, axis=1).values\n",
        "        x4 = torch.squeeze(x4)\n",
        "        x5 = torch.mean(encoded2, axis=1)\n",
        "        x5 = torch.squeeze(x5)\n",
        "\n",
        "        x6 = torch.max(encoded3, axis=1).values\n",
        "        x6 = torch.squeeze(x6)\n",
        "        x7 = torch.mean(encoded3, axis=1)\n",
        "        x7 = torch.squeeze(x7)\n",
        "\n",
        "        x8 = torch.max(encoded, axis=1).values\n",
        "        x8 = torch.squeeze(x8)\n",
        "\n",
        "        x = torch.cat([x0,x2,x4,x6,x8],-1)\n",
        "        x = self._head(x)\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQrNU87sEKT1",
        "outputId": "3ae053ad-e9d2-4e00-878a-de5b98da2f07"
      },
      "source": [
        "import joblib \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "import tqdm\n",
        "\n",
        "WIN_SIZE = 25\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "ifold = 0\n",
        "fold_metrics = []\n",
        "for tr_id, va_id in kf.split(df['id'].unique()):\n",
        "    train_df = df.loc[df['id'].isin(tr_id)].reset_index(drop=True)\n",
        "    valid_df = df.loc[df['id'].isin(va_id)].reset_index(drop=True)\n",
        "\n",
        "    train_ds = CardioDataset(train_df, WIN_SIZE)\n",
        "    valid_ds = CardioDataset(valid_df, WIN_SIZE)\n",
        "\n",
        "    train_sampler = RandomSampler(train_ds)\n",
        "    valid_sampler = SequentialSampler(valid_ds)\n",
        "    batch_size = 256\n",
        "    train_dl = DataLoader(train_ds, sampler=train_sampler, batch_size=batch_size, num_workers=0)\n",
        "    valid_dl = DataLoader(valid_ds, sampler=valid_sampler, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "    model = CardioCnn(WIN_SIZE, WIN_SIZE).to(device)\n",
        "\n",
        "    nepochs = 2\n",
        "\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                          lr = 1e-3                          \n",
        "                       )\n",
        "    optimizer = Lookahead(optimizer, 0.5, 6)\n",
        "    #optimizer = RangerLars(model.parameters())\n",
        "\n",
        "    for epoch in range(nepochs):\n",
        "        model.train();\n",
        "        optimizer.zero_grad()\n",
        "        i = 0\n",
        "        train_losses = []\n",
        "        for x in train_dl:\n",
        "            out = model(x['x'].float().to(device))\n",
        "            loss = criterion(out, x['y'].float().to(device)) \n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "            optimizer.zero_grad()\n",
        "            i += 1\n",
        "        #torch.save(model.state_dict(), 'model_'+str(epoch) + '.pth')\n",
        "            train_losses.append(loss.item())\n",
        "        \n",
        "        valid_losses = []\n",
        "        valid_df_predict = np.zeros((valid_df.shape[0],WIN_SIZE))\n",
        "        model.eval();\n",
        "        for xx in valid_dl:\n",
        "            out = model(xx['x'].float().to(device))\n",
        "            valid_loss = criterion(out, xx['y'].float().to(device)) \n",
        "            valid_losses.append(valid_loss.item())\n",
        "            out = torch.nn.Softmax(dim=1)(out)\n",
        "            for j in range(xx['x'].shape[0]):\n",
        "                i0,i1 = xx[\"start\"][j],xx[\"end\"][j]\n",
        "                s = i1 - i0\n",
        "                valid_df_predict[i0:i1, i0 % WIN_SIZE] = out[j].detach().cpu().numpy()[-s:]\n",
        "        y_proba = np.amax(valid_df_predict, 1)\n",
        "        y_true = valid_df.y.values\n",
        "\n",
        "        #best_th , best_score = threshold_search(y_true, y_proba)\n",
        "        best_th = 0.066\n",
        "        best_score = f1_score(y_true.astype(int), (y_proba > best_th).astype(int)) #, average='micro'\n",
        "        print('fold', ifold, 'epoch', epoch, 'best_th', best_th, 'f1-score', best_score) #'step', i,\n",
        "        print('fold', ifold, 'epoch', epoch, 'train_loss', np.mean(train_losses), 'valid_loss', np.mean(valid_losses)) \n",
        "        model.train();\n",
        "\n",
        "    fold_metrics.append(best_score)\n",
        "    ifold += 1"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 0 epoch 0 best_th 0.066 f1-score 0.1290704558910598\n",
            "fold 0 epoch 0 train_loss 0.44643505993268345 valid_loss 0.38455644838119807\n",
            "fold 0 epoch 1 best_th 0.066 f1-score 0.5797485109199206\n",
            "fold 0 epoch 1 train_loss 0.34427988756176653 valid_loss 0.29045598806911394\n",
            "fold 1 epoch 0 best_th 0.066 f1-score 0.003997335109926716\n",
            "fold 1 epoch 0 train_loss 0.4572673811428789 valid_loss 0.3307643223042582\n",
            "fold 1 epoch 1 best_th 0.066 f1-score 0.5172087148721187\n",
            "fold 1 epoch 1 train_loss 0.38047993247923645 valid_loss 0.2679646294198784\n",
            "fold 2 epoch 0 best_th 0.066 f1-score 0.023994354269583625\n",
            "fold 2 epoch 0 train_loss 0.4489609327995115 valid_loss 0.3395664181974199\n",
            "fold 2 epoch 1 best_th 0.066 f1-score 0.4728761514841351\n",
            "fold 2 epoch 1 train_loss 0.3889163606282737 valid_loss 0.2701900555027856\n",
            "fold 3 epoch 0 best_th 0.066 f1-score 0.0\n",
            "fold 3 epoch 0 train_loss 0.410651409486581 valid_loss 0.679981196920077\n",
            "fold 3 epoch 1 best_th 0.066 f1-score 0.6426553672316384\n",
            "fold 3 epoch 1 train_loss 0.32199000074849493 valid_loss 0.4992562391691738\n",
            "fold 4 epoch 0 best_th 0.066 f1-score 0.0\n",
            "fold 4 epoch 0 train_loss 0.44624592847381994 valid_loss 0.4036430332221483\n",
            "fold 4 epoch 1 best_th 0.066 f1-score 0.6319165998396151\n",
            "fold 4 epoch 1 train_loss 0.3618635004324629 valid_loss 0.30202011734639345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y1ax-xKfGNk",
        "outputId": "0b646167-0bd2-4a3e-d2eb-3ea4cbff415d"
      },
      "source": [
        "print(fold_metrics, np.mean(fold_metrics))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5797485109199206, 0.5172087148721187, 0.4728761514841351, 0.6426553672316384, 0.6319165998396151] 0.5688810688694856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_zl3iAfd85Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}