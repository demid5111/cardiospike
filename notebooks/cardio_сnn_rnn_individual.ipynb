{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cardio_сnn_rnn_individual.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdyY-y5mXtTM",
        "outputId": "c4ddb942-50df-4a42-82e2-4a11ccb28f61"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 12 19:33:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCRvbEjoppx0",
        "outputId": "153c2b2a-5ab4-4883-a2e9-8ea29de5059a"
      },
      "source": [
        "#https://drive.google.com/file/d/1EVwP6MUQAeSnGM-ywBRHspCn_OWEfzJN/view?usp=sharing\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1EVwP6MUQAeSnGM-ywBRHspCn_OWEfzJN\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1EVwP6MUQAeSnGM-ywBRHspCn_OWEfzJN\" -o train.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    259      0 --:--:--  0:00:01 --:--:--   259\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100  955k  100  955k    0     0   389k      0  0:00:02  0:00:02 --:--:--  389k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGCxbyAlM7Hc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "T_Xf4egHNAk-",
        "outputId": "6f5eb25a-e9ac-4a2f-b681-6ee1a5aaee5b"
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>800</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>780</td>\n",
              "      <td>780</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1572</td>\n",
              "      <td>792</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2392</td>\n",
              "      <td>820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>3196</td>\n",
              "      <td>804</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  time    x  y\n",
              "0   1     0  800  0\n",
              "1   1   780  780  0\n",
              "2   1  1572  792  0\n",
              "3   1  2392  820  0\n",
              "4   1  3196  804  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUdZ_3RoNiji",
        "outputId": "9e7c643b-e28d-42f7-90b6-45faba522f8a"
      },
      "source": [
        "df['marker'] = np.multiply(df[['id','y']].groupby('id').agg(np.cumsum).values.ravel(), df['y'].values)\n",
        "df['start'] = ((df['y'].shift(1, fill_value=0) == 0).values & (df['y'] == 1)).values\n",
        "df['end'] = ((df['y'].shift(-1, fill_value=0) == 0).values & (df['y'] == 1)).values\n",
        "\n",
        "q = df.loc[df['end'],'marker'] - df.loc[df['end'],'start']\n",
        "print('аномалий: ', q.shape[0], 'средняя длина: ', int(q.mean()*100.0)*0.01, 'min: ', q.min(), 'max: ', q.max())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "аномалий:  791 средняя длина:  25.060000000000002 min:  6 max:  81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "wH4pJnQwTsw4",
        "outputId": "ba6210e8-5381-4622-9e69-84386bd196fc"
      },
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "for q in [163]:\n",
        "    t = df.loc[df['id'] == q].sort_values('time').reset_index(drop=True)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=t['time'], y=t['x'] - t['x'].mean(),\n",
        "                        mode='lines',\n",
        "                        name='lines'))\n",
        "    qt = t.loc[t.y==1].reset_index(drop=True)\n",
        "    fig.add_trace(go.Scatter(x=qt['time'], y=qt['x'] - t['x'].mean(),\n",
        "                        mode='markers', name='markers'))\n",
        "    print(t.x.mean())\n",
        "    fig.show()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "728.0415584415584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1aa932ed-109d-4e59-a31a-cd393a662778\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1aa932ed-109d-4e59-a31a-cd393a662778\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1aa932ed-109d-4e59-a31a-cd393a662778',\n",
              "                        [{\"mode\": \"lines\", \"name\": \"lines\", \"type\": \"scatter\", \"x\": [0, 400, 568, 1336, 1844, 1992, 2472, 3232, 4080, 4704, 5668, 5840, 6084, 6372, 6480, 7228, 8036, 8836, 9408, 9636, 9856, 10460, 11304, 12180, 13068, 13948, 14844, 15720, 16592, 17448, 18300, 19148, 19988, 20788, 21684, 22456, 23268, 24132, 24912, 25780, 26036, 26572, 27412, 28260, 29084, 29940, 30796, 31700, 32484, 33320, 34160, 35048, 35832, 36272, 36668, 37560, 38392, 39312, 39548, 40044, 40816, 41636, 42612, 42724, 43300, 44176, 44504, 44988, 45792, 46200, 46576, 47392, 48200, 48848, 49072, 49828, 50628, 51412, 52188, 52968, 53752, 54532, 55304, 56112, 56900, 57712, 58540, 59400, 60176, 61024, 61848, 62672, 63492, 64320, 65148, 65952, 66764, 67580, 68396, 69248, 70012, 70828, 71688, 72452, 73280, 74160, 74936, 75820, 76664, 77460, 78276, 79164, 79952, 80492, 80836, 81608, 82456, 83292, 84132, 84704, 84960, 85416, 85816, 86668, 87500, 88344, 89200, 90040, 90880, 91736, 92588, 93420, 94264, 95120, 95968, 96800, 97656, 98516, 99416, 100208, 101072, 101944, 102792, 103632, 104488, 105404, 106204, 107052, 107920, 108784, 109636, 110488, 111356, 112236, 113104, 113964, 114820, 115660, 116484, 117316, 118172, 119076, 119896, 120744, 121612, 122496, 123416, 124216, 125100, 125996, 126864, 127760, 128668, 129560, 130436, 131332, 132236, 133128, 134016, 134924, 135844, 136748, 137632, 138544, 139472, 140428, 141276, 142212, 143152, 144072, 145028, 145856, 146756, 147716, 148588, 149504, 150440, 151288, 152196, 153088, 154004, 154944, 155868, 156768, 157676, 158592, 159512, 160452, 161312, 162236, 163148, 164036, 164964, 165888, 166784, 167648, 168496, 169368, 170252, 171128, 172032, 172952, 173884, 174792, 175700, 176628, 177536, 178444, 179368, 180292, 181184, 182100, 183028, 183960, 184876, 185812, 186720, 187636, 188556, 189460, 190372, 191296, 192288, 193144, 194060, 195012, 195952, 196916, 197772, 198688, 199608, 200512, 201456, 202388, 203292, 204232, 205156, 206136, 206996, 207904, 208812, 209732, 210636, 211540, 212468, 213396, 214308, 215236, 216168, 217084, 218044, 218872, 219756, 220656, 221540, 222472, 223304, 224196, 225072, 225980, 227692, 228044, 228500, 229396, 230252, 231136, 232004, 232808, 232992, 233712, 234520, 234924, 235204, 235496, 236080, 236236, 236580, 236668, 237076, 237172, 237560, 237712, 237904, 238768, 239388, 239920, 240408, 241512, 242452, 242708, 242936, 243056, 243756, 244576, 245416, 246200, 247076, 247872, 248772, 249568, 250416, 251256, 252128, 252944, 253780, 254548, 255424, 256212, 257100, 257904, 258752, 259608, 260448, 261308, 262156, 263012, 263192, 263856, 264256, 264488, 264740, 265060, 265264, 265576, 265936, 266372, 267200, 268036, 268680, 268864, 269700, 269828, 270584, 271356, 271660, 273016, 273832, 274496, 274664, 275488, 275880, 276308, 277124, 277932, 278740, 279464, 279648, 280472, 280520, 280884, 281156, 281324, 281952, 282184, 282396, 282752, 283548, 284348, 285160, 285964, 286768, 287564, 288356, 289140, 289928, 290712, 291500, 292328, 293108, 293892, 294680, 295424, 296216, 297012, 297808, 298608, 299408, 300260, 301056, 301860, 302668, 303424, 304220, 305072, 305832, 306636, 307448, 308308, 309120, 309920, 310720, 311540, 312356, 313128, 313952, 314812, 315636, 316468, 317244, 318116, 318896, 319760, 320576, 321404, 322172, 322984, 323844, 324612, 325468, 326236, 327120, 327904, 328780, 329548, 330372, 331188, 332000, 332820, 333632, 334440, 335292, 335368, 336040, 336832, 337636, 338652, 338876, 339340, 339940, 340188, 341020, 341304, 341716, 341812, 342100, 342384, 342692, 343212, 343588, 344456, 345356, 346192, 347024, 347896, 348752, 349648, 349812, 349968, 350444, 351348, 352152, 353000, 354712, 355572, 356372, 357276, 358020, 358324, 358564, 358916, 359204, 359708, 360080, 360540, 360792, 360988, 361244, 362232, 362360, 363040, 363088, 363880, 364532, 364712, 364972, 365588, 366376, 367204, 368040, 368364, 368528, 368876, 369372, 369712, 370540, 371372, 372212, 373048, 373880, 374724, 375560, 376400, 377192, 377248, 377708, 377828, 378068, 378632, 378888, 379708, 380576, 381352, 382176, 383012, 383836, 384656, 385480, 386312, 387136, 387972, 388840, 389252, 389680, 390500, 390728, 391012, 391332, 392152, 392920, 393792, 394556, 394680, 394976, 395420, 396232, 396372, 397040, 397800, 398616, 399460, 400260, 401008, 401460, 401712, 402196, 402572, 403384, 403524, 403800, 404708, 404884, 405036, 405664, 407224, 408012, 408304, 408804, 409592, 410384, 411184, 411756, 411984, 413572, 414224, 414496, 415072, 415568, 415764, 416964, 417196, 417540, 418236, 419000, 419756, 420512, 421268, 422068, 422808, 423228, 423508, 424244, 424420, 424984, 425388, 425728, 426516, 427212, 427924, 428736, 429056, 429076, 429488, 429664, 429800, 430232, 430400, 430528, 430776, 431132, 431324, 431656, 431832, 432060, 432472, 433176, 433932, 434088, 434672, 435116, 435424, 436176, 436968, 437468, 437672, 438416, 439164, 439900, 439972, 440256, 440640, 440832, 441384, 442120, 442872, 443632, 444396, 445172, 445996, 446724, 447500, 448272, 448648, 448668, 449036, 449808, 450580, 451356, 452120, 452892, 453680, 454460, 455248, 456024, 456804, 457596, 458384, 459156, 459984, 460720, 461544, 462276, 462816, 463056, 463952, 464864, 465900, 466756, 467688, 468688, 469600, 469956, 470104, 470356, 470760, 471308, 471668, 472180, 472760, 473216, 474772, 475628, 475920, 475936, 476504, 477552, 477732, 477896, 478164, 478708, 478992, 479384, 479484, 479852, 480040, 480296, 480532, 480688, 481244, 481500, 482332, 482536, 482644, 483168, 483724, 484048, 484164, 484804, 485660, 485888, 486512, 486804, 486960, 487232, 487392, 488608, 488816, 490008, 490200, 490400, 490624, 490984, 491076, 491480, 492248, 492332, 493116, 493508, 493672, 493864, 494680, 494800, 495228, 495392, 495668, 496496, 496824, 498156, 498956, 499756, 500540, 501332, 502136, 502936, 503788, 504716, 505704, 506536, 507452, 508336, 509272, 510088, 510940, 511852, 512732, 513248, 513644, 514472, 515380, 516228, 517072, 517968, 518876, 519152, 519716, 520172, 520552, 521476, 522280, 523236, 523320, 523748, 524036, 524392, 524560, 524808, 525224, 525420, 525680, 525928, 526236, 526456, 526552, 527424, 528260, 528940, 529172, 529216, 529996, 530836, 531208, 531468, 531680, 532076, 532552, 533404, 534236, 535092, 535948, 536808, 537676, 538524, 539372, 540232, 541076, 541944, 542848, 543648, 544564, 545380, 546304, 547128, 547992, 548864, 549772, 550636, 551436, 552300, 553156, 554016, 554904, 555808, 556668, 557528, 558348, 559252, 560104, 560960, 561816, 562664, 563500, 564348, 565196, 566036, 566868, 567716, 568556, 569388, 570192, 571092, 571936, 572744, 573652, 574508, 576152, 577048, 577844, 578704, 579540, 580364, 581200, 581776, 582032, 582708, 582892, 583100, 583592, 584024, 584132, 584508, 584952, 585200, 585536, 585988, 586724, 586832, 587676, 588084, 588332, 588500, 589300, 590188, 590992, 591816, 592588, 593444, 594204, 595052, 596076, 596952, 597896, 598712, 599580, 599852, 599924, 600244, 600552, 601380, 601668, 602284, 603076, 603988, 604600, 604780, 605604, 606456, 607332, 608188, 609032, 609916, 610772, 611612, 612080, 612484, 613340, 614172, 615052, 615960, 616796, 617612, 618232, 618360, 618888, 619244, 619320, 621104, 621968, 622300, 622808, 623648, 623924, 624380, 624520, 625160, 625372, 625684, 625812, 626208, 626588, 626924, 627048, 627876, 628700, 629528, 630292, 631160, 631956, 632756, 633548, 634304, 635108, 635916, 636772, 637576, 638356, 639184, 640064, 640900, 641704, 642556, 643016, 643472, 644308, 645108, 645944, 646788, 647620, 648448, 649268, 650076, 650876, 651672, 652540, 653356, 654188, 654968, 655868, 656716, 657516, 658388, 659296, 660096, 660964, 661868, 662708, 663560, 664404, 665248, 666092, 666932, 667768, 668608, 669436, 670276, 671120, 671960, 672768, 673624, 674516, 675368, 676212, 677052, 677884, 678068, 678712, 678992, 679468, 680132, 680332, 681136, 681452, 681640, 681868, 682600, 682828, 683580, 684408, 685240, 686064, 686864, 687636, 688484, 689064, 689368, 690176, 691000, 691776, 692648, 693428, 694284, 694848, 695936, 696244, 696612, 696772, 697580, 698412, 699236, 700048, 700368, 700876, 701300, 701380, 701688, 702496, 703304, 704104, 704728, 704836, 705792, 706556, 707384, 708208, 709036, 709844, 710676, 711504, 712120, 712500, 713144, 713964, 714772, 715568, 716372, 717164, 717944, 718728, 719512, 720304, 721100, 721892, 722732, 723476, 724344, 725128, 725948, 726500, 726832, 727664, 728496, 729276, 729928, 730460, 730996, 771896, 772728, 773544, 774368, 775196, 776020, 776844, 777660, 778476, 779248, 780076, 780940, 781752, 782580, 783364, 784192, 785068, 785844, 786688, 787576, 788408, 789236, 790068, 790896, 791684, 792556, 793380, 794204, 794980, 795852, 796680, 797500, 798320, 799156, 800000, 800840, 801636, 802520, 803364, 804192, 805044, 805848, 806736, 807588, 808436, 809280, 810124, 810952, 811776, 812608, 813456, 814300, 815176, 816032, 816884, 817732, 818596, 819460, 820328, 821184, 822032, 822896, 823740, 824592, 825452, 826300, 827160, 828020, 828884, 829732, 830592, 831448, 832308, 833140, 834036, 834892, 835740, 836592, 837444, 838252, 839152, 839992], \"y\": [167.95844155844156, -328.04155844155844, -560.0415584415584, 39.958441558441564, -220.04155844155844, -580.0415584415584, -248.04155844155844, 31.958441558441564, 119.95844155844156, -104.04155844155844, 235.95844155844156, -556.0415584415584, -484.04155844155844, -440.04155844155844, -620.0415584415584, 19.958441558441564, 79.95844155844156, 71.95844155844156, -156.04155844155844, -500.04155844155844, -508.04155844155844, -124.04155844155844, 115.95844155844156, 147.95844155844156, 159.95844155844156, 151.95844155844156, 167.95844155844156, 147.95844155844156, 143.95844155844156, 127.95844155844156, 123.95844155844156, 119.95844155844156, 111.95844155844156, 71.95844155844156, 167.95844155844156, 43.958441558441564, 83.95844155844156, 135.95844155844156, 51.958441558441564, 139.95844155844156, -472.04155844155844, -192.04155844155844, 111.95844155844156, 119.95844155844156, 95.95844155844156, 127.95844155844156, 127.95844155844156, 175.95844155844156, 55.958441558441564, 107.95844155844156, 111.95844155844156, 159.95844155844156, 55.958441558441564, -288.04155844155844, -332.04155844155844, 163.95844155844156, 103.95844155844156, 191.95844155844156, -492.04155844155844, -232.04155844155844, 43.958441558441564, 91.95844155844156, 247.95844155844156, -616.0415584415584, -152.04155844155844, 147.95844155844156, -400.04155844155844, -244.04155844155844, 75.95844155844156, -320.04155844155844, -352.04155844155844, 87.95844155844156, 79.95844155844156, -80.04155844155844, -504.04155844155844, 27.958441558441564, 71.95844155844156, 55.958441558441564, 47.958441558441564, 51.958441558441564, 55.958441558441564, 51.958441558441564, 43.958441558441564, 79.95844155844156, 59.958441558441564, 83.95844155844156, 99.95844155844156, 131.95844155844156, 47.958441558441564, 119.95844155844156, 95.95844155844156, 95.95844155844156, 91.95844155844156, 99.95844155844156, 99.95844155844156, 75.95844155844156, 83.95844155844156, 87.95844155844156, 87.95844155844156, 123.95844155844156, 35.958441558441564, 87.95844155844156, 131.95844155844156, 35.958441558441564, 99.95844155844156, 151.95844155844156, 47.958441558441564, 155.95844155844156, 115.95844155844156, 67.95844155844156, 87.95844155844156, 159.95844155844156, 59.958441558441564, -188.04155844155844, -384.04155844155844, 43.958441558441564, 119.95844155844156, 107.95844155844156, 111.95844155844156, -156.04155844155844, -472.04155844155844, -272.04155844155844, -328.04155844155844, 123.95844155844156, 103.95844155844156, 115.95844155844156, 127.95844155844156, 111.95844155844156, 111.95844155844156, 127.95844155844156, 123.95844155844156, 103.95844155844156, 115.95844155844156, 127.95844155844156, 119.95844155844156, 103.95844155844156, 127.95844155844156, 131.95844155844156, 171.95844155844156, 63.958441558441564, 135.95844155844156, 143.95844155844156, 119.95844155844156, 111.95844155844156, 127.95844155844156, 187.95844155844156, 71.95844155844156, 119.95844155844156, 139.95844155844156, 135.95844155844156, 123.95844155844156, 123.95844155844156, 139.95844155844156, 151.95844155844156, 139.95844155844156, 131.95844155844156, 127.95844155844156, 111.95844155844156, 95.95844155844156, 103.95844155844156, 127.95844155844156, 175.95844155844156, 91.95844155844156, 119.95844155844156, 139.95844155844156, 155.95844155844156, 191.95844155844156, 71.95844155844156, 155.95844155844156, 167.95844155844156, 139.95844155844156, 167.95844155844156, 179.95844155844156, 163.95844155844156, 147.95844155844156, 167.95844155844156, 175.95844155844156, 163.95844155844156, 159.95844155844156, 179.95844155844156, 191.95844155844156, 175.95844155844156, 155.95844155844156, 183.95844155844156, 199.95844155844156, 227.95844155844156, 119.95844155844156, 207.95844155844156, 211.95844155844156, 191.95844155844156, 227.95844155844156, 99.95844155844156, 171.95844155844156, 231.95844155844156, 143.95844155844156, 187.95844155844156, 207.95844155844156, 119.95844155844156, 179.95844155844156, 163.95844155844156, 187.95844155844156, 211.95844155844156, 195.95844155844156, 171.95844155844156, 179.95844155844156, 187.95844155844156, 191.95844155844156, 211.95844155844156, 131.95844155844156, 195.95844155844156, 183.95844155844156, 159.95844155844156, 199.95844155844156, 195.95844155844156, 167.95844155844156, 135.95844155844156, 119.95844155844156, 143.95844155844156, 155.95844155844156, 147.95844155844156, 175.95844155844156, 191.95844155844156, 203.95844155844156, 179.95844155844156, 179.95844155844156, 199.95844155844156, 179.95844155844156, 179.95844155844156, 195.95844155844156, 195.95844155844156, 163.95844155844156, 187.95844155844156, 199.95844155844156, 203.95844155844156, 187.95844155844156, 207.95844155844156, 179.95844155844156, 187.95844155844156, 191.95844155844156, 175.95844155844156, 183.95844155844156, 195.95844155844156, 263.95844155844156, 127.95844155844156, 187.95844155844156, 223.95844155844156, 211.95844155844156, 235.95844155844156, 127.95844155844156, 187.95844155844156, 191.95844155844156, 175.95844155844156, 215.95844155844156, 203.95844155844156, 175.95844155844156, 211.95844155844156, 195.95844155844156, 251.95844155844156, 131.95844155844156, 179.95844155844156, 179.95844155844156, 191.95844155844156, 175.95844155844156, 175.95844155844156, 199.95844155844156, 199.95844155844156, 183.95844155844156, 199.95844155844156, 203.95844155844156, 187.95844155844156, 231.95844155844156, 99.95844155844156, 155.95844155844156, 171.95844155844156, 155.95844155844156, 203.95844155844156, 103.95844155844156, 163.95844155844156, 147.95844155844156, 179.95844155844156, 983.9584415584416, -376.04155844155844, -272.04155844155844, 167.95844155844156, 127.95844155844156, 155.95844155844156, 139.95844155844156, 75.95844155844156, -544.0415584415584, -8.041558441558436, 79.95844155844156, -324.04155844155844, -448.04155844155844, -436.04155844155844, -144.04155844155844, -572.0415584415584, -384.04155844155844, -640.0415584415584, -320.04155844155844, -632.0415584415584, -340.04155844155844, -576.0415584415584, -536.0415584415584, 135.95844155844156, -108.04155844155844, -196.04155844155844, -240.04155844155844, 375.95844155844156, 211.95844155844156, -472.04155844155844, -500.04155844155844, -608.0415584415584, -28.041558441558436, 91.95844155844156, 111.95844155844156, 55.958441558441564, 147.95844155844156, 67.95844155844156, 171.95844155844156, 67.95844155844156, 119.95844155844156, 111.95844155844156, 143.95844155844156, 87.95844155844156, 107.95844155844156, 39.958441558441564, 147.95844155844156, 59.958441558441564, 159.95844155844156, 75.95844155844156, 119.95844155844156, 127.95844155844156, 111.95844155844156, 131.95844155844156, 119.95844155844156, 127.95844155844156, -548.0415584415584, -64.04155844155844, -328.04155844155844, -496.04155844155844, -476.04155844155844, -408.04155844155844, -524.0415584415584, -416.04155844155844, -368.04155844155844, -292.04155844155844, 99.95844155844156, 107.95844155844156, -84.04155844155844, -544.0415584415584, 107.95844155844156, -600.0415584415584, 27.958441558441564, 43.958441558441564, -424.04155844155844, 627.9584415584416, 87.95844155844156, -64.04155844155844, -560.0415584415584, 95.95844155844156, -336.04155844155844, -300.04155844155844, 87.95844155844156, 79.95844155844156, 79.95844155844156, -4.041558441558436, -544.0415584415584, 95.95844155844156, -680.0415584415584, -364.04155844155844, -456.04155844155844, -560.0415584415584, -100.04155844155844, -496.04155844155844, -516.0415584415584, -372.04155844155844, 67.95844155844156, 71.95844155844156, 83.95844155844156, 75.95844155844156, 75.95844155844156, 67.95844155844156, 63.958441558441564, 55.958441558441564, 59.958441558441564, 55.958441558441564, 59.958441558441564, 99.95844155844156, 51.958441558441564, 55.958441558441564, 59.958441558441564, 15.958441558441564, 63.958441558441564, 67.95844155844156, 67.95844155844156, 71.95844155844156, 71.95844155844156, 123.95844155844156, 67.95844155844156, 75.95844155844156, 79.95844155844156, 27.958441558441564, 67.95844155844156, 123.95844155844156, 31.958441558441564, 75.95844155844156, 83.95844155844156, 131.95844155844156, 83.95844155844156, 71.95844155844156, 71.95844155844156, 91.95844155844156, 87.95844155844156, 43.958441558441564, 95.95844155844156, 131.95844155844156, 95.95844155844156, 103.95844155844156, 47.958441558441564, 143.95844155844156, 51.958441558441564, 135.95844155844156, 87.95844155844156, 99.95844155844156, 39.958441558441564, 83.95844155844156, 131.95844155844156, 39.958441558441564, 127.95844155844156, 39.958441558441564, 155.95844155844156, 55.958441558441564, 147.95844155844156, 39.958441558441564, 95.95844155844156, 87.95844155844156, 83.95844155844156, 91.95844155844156, 83.95844155844156, 79.95844155844156, 123.95844155844156, -652.0415584415584, -56.041558441558436, 63.958441558441564, 75.95844155844156, 287.95844155844156, -504.04155844155844, -264.04155844155844, -128.04155844155844, -480.04155844155844, 103.95844155844156, -444.04155844155844, -316.04155844155844, -632.0415584415584, -440.04155844155844, -444.04155844155844, -420.04155844155844, -208.04155844155844, -352.04155844155844, 139.95844155844156, 171.95844155844156, 107.95844155844156, 103.95844155844156, 143.95844155844156, 127.95844155844156, 167.95844155844156, -564.0415584415584, -572.0415584415584, -252.04155844155844, 175.95844155844156, 75.95844155844156, 119.95844155844156, 983.9584415584416, 131.95844155844156, 71.95844155844156, 175.95844155844156, 15.958441558441564, -424.04155844155844, -488.04155844155844, -376.04155844155844, -440.04155844155844, -224.04155844155844, -356.04155844155844, -268.04155844155844, -476.04155844155844, -532.0415584415584, -472.04155844155844, 259.95844155844156, -600.0415584415584, -48.041558441558436, -680.0415584415584, 63.958441558441564, -76.04155844155844, -548.0415584415584, -468.04155844155844, -112.04155844155844, 59.958441558441564, 99.95844155844156, 107.95844155844156, -404.04155844155844, -564.0415584415584, -380.04155844155844, -232.04155844155844, -388.04155844155844, 99.95844155844156, 103.95844155844156, 111.95844155844156, 107.95844155844156, 103.95844155844156, 115.95844155844156, 107.95844155844156, 111.95844155844156, 63.958441558441564, -672.0415584415584, -268.04155844155844, -608.0415584415584, -488.04155844155844, -164.04155844155844, -472.04155844155844, 91.95844155844156, 139.95844155844156, 47.958441558441564, 95.95844155844156, 107.95844155844156, 95.95844155844156, 91.95844155844156, 95.95844155844156, 103.95844155844156, 95.95844155844156, 107.95844155844156, 139.95844155844156, -316.04155844155844, -300.04155844155844, 91.95844155844156, -500.04155844155844, -444.04155844155844, -408.04155844155844, 91.95844155844156, 39.958441558441564, 143.95844155844156, 35.958441558441564, -604.0415584415584, -432.04155844155844, -284.04155844155844, 83.95844155844156, -588.0415584415584, -60.041558441558436, 31.958441558441564, 87.95844155844156, 115.95844155844156, 71.95844155844156, 19.958441558441564, -276.04155844155844, -476.04155844155844, -244.04155844155844, -352.04155844155844, 83.95844155844156, -588.0415584415584, -452.04155844155844, 179.95844155844156, -552.0415584415584, -576.0415584415584, -100.04155844155844, 831.9584415584416, 59.958441558441564, -436.04155844155844, -228.04155844155844, 59.958441558441564, 63.958441558441564, 71.95844155844156, -156.04155844155844, -500.04155844155844, 859.9584415584416, -76.04155844155844, -456.04155844155844, -152.04155844155844, -232.04155844155844, -532.0415584415584, 471.95844155844156, -496.04155844155844, -384.04155844155844, -32.041558441558436, 35.958441558441564, 27.958441558441564, 27.958441558441564, 27.958441558441564, 71.95844155844156, 11.958441558441564, -308.04155844155844, -448.04155844155844, 7.958441558441564, -552.0415584415584, -164.04155844155844, -324.04155844155844, -388.04155844155844, 59.958441558441564, -32.041558441558436, -16.041558441558436, 83.95844155844156, -408.04155844155844, -708.0415584415584, -316.04155844155844, -552.0415584415584, -592.0415584415584, -296.04155844155844, -560.0415584415584, -600.0415584415584, -480.04155844155844, -372.04155844155844, -536.0415584415584, -396.04155844155844, -552.0415584415584, -500.04155844155844, -316.04155844155844, -24.041558441558436, 27.958441558441564, -572.0415584415584, -144.04155844155844, -284.04155844155844, -420.04155844155844, 23.958441558441564, 63.958441558441564, -228.04155844155844, -524.0415584415584, 15.958441558441564, 19.958441558441564, 7.958441558441564, -656.0415584415584, -444.04155844155844, -344.04155844155844, -536.0415584415584, -176.04155844155844, 7.958441558441564, 23.958441558441564, 31.958441558441564, 35.958441558441564, 47.958441558441564, 95.95844155844156, -0.04155844155843624, 47.958441558441564, 43.958441558441564, -352.04155844155844, -708.0415584415584, -360.04155844155844, 43.958441558441564, 43.958441558441564, 47.958441558441564, 35.958441558441564, 43.958441558441564, 59.958441558441564, 51.958441558441564, 59.958441558441564, 47.958441558441564, 51.958441558441564, 63.958441558441564, 59.958441558441564, 43.958441558441564, 99.95844155844156, 7.958441558441564, 95.95844155844156, 3.9584415584415638, -188.04155844155844, -488.04155844155844, 167.95844155844156, 183.95844155844156, 307.95844155844156, 127.95844155844156, 203.95844155844156, 271.95844155844156, 183.95844155844156, -372.04155844155844, -580.0415584415584, -476.04155844155844, -324.04155844155844, -180.04155844155844, -368.04155844155844, -216.04155844155844, -148.04155844155844, -272.04155844155844, 827.9584415584416, 127.95844155844156, -436.04155844155844, -712.0415584415584, -160.04155844155844, 319.95844155844156, -548.0415584415584, -564.0415584415584, -460.04155844155844, -184.04155844155844, -444.04155844155844, -336.04155844155844, -628.0415584415584, -360.04155844155844, -540.0415584415584, -472.04155844155844, -492.04155844155844, -572.0415584415584, -172.04155844155844, -472.04155844155844, 103.95844155844156, -524.0415584415584, -620.0415584415584, -204.04155844155844, -172.04155844155844, -404.04155844155844, -612.0415584415584, -88.04155844155844, 127.95844155844156, -500.04155844155844, -104.04155844155844, -436.04155844155844, -572.0415584415584, -456.04155844155844, -568.0415584415584, 487.95844155844156, -520.0415584415584, 463.95844155844156, -536.0415584415584, -528.0415584415584, -504.04155844155844, -368.04155844155844, -636.0415584415584, -324.04155844155844, 39.958441558441564, -644.0415584415584, 55.958441558441564, -336.04155844155844, -564.0415584415584, -536.0415584415584, 87.95844155844156, -608.0415584415584, -300.04155844155844, -564.0415584415584, -452.04155844155844, 99.95844155844156, -400.04155844155844, 603.9584415584416, 71.95844155844156, 71.95844155844156, 55.958441558441564, 63.958441558441564, 75.95844155844156, 71.95844155844156, 123.95844155844156, 199.95844155844156, 259.95844155844156, 103.95844155844156, 187.95844155844156, 155.95844155844156, 207.95844155844156, 87.95844155844156, 123.95844155844156, 183.95844155844156, 151.95844155844156, -212.04155844155844, -332.04155844155844, 99.95844155844156, 179.95844155844156, 119.95844155844156, 115.95844155844156, 167.95844155844156, 179.95844155844156, -452.04155844155844, -164.04155844155844, -272.04155844155844, -348.04155844155844, 195.95844155844156, 75.95844155844156, 227.95844155844156, -644.0415584415584, -300.04155844155844, -440.04155844155844, -372.04155844155844, -560.0415584415584, -480.04155844155844, -312.04155844155844, -532.0415584415584, -468.04155844155844, -480.04155844155844, -420.04155844155844, -508.04155844155844, -632.0415584415584, 143.95844155844156, 107.95844155844156, -48.041558441558436, -496.04155844155844, -684.0415584415584, 51.958441558441564, 111.95844155844156, -356.04155844155844, -468.04155844155844, -516.0415584415584, -332.04155844155844, -252.04155844155844, 123.95844155844156, 103.95844155844156, 127.95844155844156, 127.95844155844156, 131.95844155844156, 139.95844155844156, 119.95844155844156, 119.95844155844156, 131.95844155844156, 115.95844155844156, 139.95844155844156, 175.95844155844156, 71.95844155844156, 187.95844155844156, 87.95844155844156, 195.95844155844156, 95.95844155844156, 135.95844155844156, 143.95844155844156, 179.95844155844156, 135.95844155844156, 71.95844155844156, 135.95844155844156, 127.95844155844156, 131.95844155844156, 159.95844155844156, 175.95844155844156, 131.95844155844156, 131.95844155844156, 91.95844155844156, 175.95844155844156, 123.95844155844156, 127.95844155844156, 127.95844155844156, 119.95844155844156, 107.95844155844156, 119.95844155844156, 119.95844155844156, 111.95844155844156, 103.95844155844156, 119.95844155844156, 111.95844155844156, 103.95844155844156, 75.95844155844156, 171.95844155844156, 115.95844155844156, 79.95844155844156, 179.95844155844156, 127.95844155844156, 915.9584415584416, 167.95844155844156, 67.95844155844156, 131.95844155844156, 107.95844155844156, 95.95844155844156, 107.95844155844156, -152.04155844155844, -472.04155844155844, -52.041558441558436, -544.0415584415584, -520.0415584415584, -236.04155844155844, -296.04155844155844, -620.0415584415584, -352.04155844155844, -284.04155844155844, -480.04155844155844, -392.04155844155844, -276.04155844155844, 7.958441558441564, -620.0415584415584, 115.95844155844156, -320.04155844155844, -480.04155844155844, -560.0415584415584, 71.95844155844156, 159.95844155844156, 75.95844155844156, 95.95844155844156, 43.958441558441564, 127.95844155844156, 31.958441558441564, 119.95844155844156, 295.95844155844156, 147.95844155844156, 215.95844155844156, 87.95844155844156, 139.95844155844156, -456.04155844155844, -656.0415584415584, -408.04155844155844, -420.04155844155844, 99.95844155844156, -440.04155844155844, -112.04155844155844, 63.958441558441564, 183.95844155844156, -116.04155844155844, -548.0415584415584, 95.95844155844156, 123.95844155844156, 147.95844155844156, 127.95844155844156, 115.95844155844156, 155.95844155844156, 127.95844155844156, 111.95844155844156, -260.04155844155844, -324.04155844155844, 127.95844155844156, 103.95844155844156, 151.95844155844156, 179.95844155844156, 107.95844155844156, 87.95844155844156, -108.04155844155844, -600.0415584415584, -200.04155844155844, -372.04155844155844, -652.0415584415584, 1055.9584415584416, 135.95844155844156, -396.04155844155844, -220.04155844155844, 111.95844155844156, -452.04155844155844, -272.04155844155844, -588.0415584415584, -88.04155844155844, -516.0415584415584, -416.04155844155844, -600.0415584415584, -332.04155844155844, -348.04155844155844, -392.04155844155844, -604.0415584415584, 99.95844155844156, 95.95844155844156, 99.95844155844156, 35.958441558441564, 139.95844155844156, 67.95844155844156, 71.95844155844156, 63.958441558441564, 27.958441558441564, 75.95844155844156, 79.95844155844156, 127.95844155844156, 75.95844155844156, 51.958441558441564, 99.95844155844156, 151.95844155844156, 107.95844155844156, 75.95844155844156, 123.95844155844156, -268.04155844155844, -272.04155844155844, 107.95844155844156, 71.95844155844156, 107.95844155844156, 115.95844155844156, 103.95844155844156, 99.95844155844156, 91.95844155844156, 79.95844155844156, 71.95844155844156, 67.95844155844156, 139.95844155844156, 87.95844155844156, 103.95844155844156, 51.958441558441564, 171.95844155844156, 119.95844155844156, 71.95844155844156, 143.95844155844156, 179.95844155844156, 71.95844155844156, 139.95844155844156, 175.95844155844156, 111.95844155844156, 123.95844155844156, 115.95844155844156, 115.95844155844156, 115.95844155844156, 111.95844155844156, 107.95844155844156, 111.95844155844156, 99.95844155844156, 111.95844155844156, 115.95844155844156, 111.95844155844156, 79.95844155844156, 127.95844155844156, 163.95844155844156, 123.95844155844156, 115.95844155844156, 111.95844155844156, 103.95844155844156, -544.0415584415584, -84.04155844155844, -448.04155844155844, -252.04155844155844, -64.04155844155844, -528.0415584415584, 75.95844155844156, -412.04155844155844, -540.0415584415584, -500.04155844155844, 3.9584415584415638, -500.04155844155844, 23.958441558441564, 99.95844155844156, 103.95844155844156, 95.95844155844156, 71.95844155844156, 43.958441558441564, 119.95844155844156, -148.04155844155844, -424.04155844155844, 79.95844155844156, 95.95844155844156, 47.958441558441564, 143.95844155844156, 51.958441558441564, 127.95844155844156, -164.04155844155844, 359.95844155844156, -420.04155844155844, -360.04155844155844, -568.0415584415584, 79.95844155844156, 103.95844155844156, 95.95844155844156, 83.95844155844156, -408.04155844155844, -220.04155844155844, -304.04155844155844, -648.0415584415584, -420.04155844155844, 79.95844155844156, 79.95844155844156, 71.95844155844156, -104.04155844155844, -620.0415584415584, 227.95844155844156, 35.958441558441564, 99.95844155844156, 95.95844155844156, 99.95844155844156, 79.95844155844156, 103.95844155844156, 99.95844155844156, -112.04155844155844, -348.04155844155844, -84.04155844155844, 91.95844155844156, 79.95844155844156, 67.95844155844156, 75.95844155844156, 63.958441558441564, 51.958441558441564, 55.958441558441564, 55.958441558441564, 63.958441558441564, 67.95844155844156, 63.958441558441564, 111.95844155844156, 15.958441558441564, 139.95844155844156, 55.958441558441564, 91.95844155844156, -176.04155844155844, -396.04155844155844, 103.95844155844156, 103.95844155844156, 51.958441558441564, -76.04155844155844, -196.04155844155844, -192.04155844155844, 40171.95844155844, 103.95844155844156, 87.95844155844156, 95.95844155844156, 99.95844155844156, 95.95844155844156, 95.95844155844156, 87.95844155844156, 87.95844155844156, 43.958441558441564, 99.95844155844156, 135.95844155844156, 83.95844155844156, 99.95844155844156, 55.958441558441564, 99.95844155844156, 147.95844155844156, 47.958441558441564, 115.95844155844156, 159.95844155844156, 103.95844155844156, 99.95844155844156, 103.95844155844156, 99.95844155844156, 59.958441558441564, 143.95844155844156, 95.95844155844156, 95.95844155844156, 47.958441558441564, 143.95844155844156, 99.95844155844156, 91.95844155844156, 91.95844155844156, 107.95844155844156, 115.95844155844156, 111.95844155844156, 67.95844155844156, 155.95844155844156, 115.95844155844156, 99.95844155844156, 123.95844155844156, 75.95844155844156, 159.95844155844156, 123.95844155844156, 119.95844155844156, 115.95844155844156, 115.95844155844156, 99.95844155844156, 95.95844155844156, 103.95844155844156, 119.95844155844156, 115.95844155844156, 147.95844155844156, 127.95844155844156, 123.95844155844156, 119.95844155844156, 135.95844155844156, 135.95844155844156, 139.95844155844156, 127.95844155844156, 119.95844155844156, 135.95844155844156, 115.95844155844156, 123.95844155844156, 131.95844155844156, 119.95844155844156, 131.95844155844156, 131.95844155844156, 135.95844155844156, 119.95844155844156, 131.95844155844156, 127.95844155844156, 131.95844155844156, 103.95844155844156, 167.95844155844156, 127.95844155844156, 119.95844155844156, 123.95844155844156, 123.95844155844156, 79.95844155844156, 171.95844155844156, 111.95844155844156]}, {\"mode\": \"markers\", \"name\": \"markers\", \"type\": \"scatter\", \"x\": [95968, 96800, 97656, 98516, 99416, 100208, 101072, 101944, 102792, 103632, 104488, 105404, 106204, 107052, 107920, 108784, 109636, 187636, 188556, 189460, 190372, 191296, 192288, 193144, 194060, 195012, 195952, 203292, 204232, 205156, 206136, 206996, 207904, 208812, 209732, 210636, 211540, 212468, 214308, 215236, 216168, 217084, 218044, 218872, 219756, 220656, 221540, 222472, 223304, 224196, 225072], \"y\": [119.95844155844156, 103.95844155844156, 127.95844155844156, 131.95844155844156, 171.95844155844156, 63.958441558441564, 135.95844155844156, 143.95844155844156, 119.95844155844156, 111.95844155844156, 127.95844155844156, 187.95844155844156, 71.95844155844156, 119.95844155844156, 139.95844155844156, 135.95844155844156, 123.95844155844156, 187.95844155844156, 191.95844155844156, 175.95844155844156, 183.95844155844156, 195.95844155844156, 263.95844155844156, 127.95844155844156, 187.95844155844156, 223.95844155844156, 211.95844155844156, 175.95844155844156, 211.95844155844156, 195.95844155844156, 251.95844155844156, 131.95844155844156, 179.95844155844156, 179.95844155844156, 191.95844155844156, 175.95844155844156, 175.95844155844156, 199.95844155844156, 183.95844155844156, 199.95844155844156, 203.95844155844156, 187.95844155844156, 231.95844155844156, 99.95844155844156, 155.95844155844156, 171.95844155844156, 155.95844155844156, 203.95844155844156, 103.95844155844156, 163.95844155844156, 147.95844155844156]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1aa932ed-109d-4e59-a31a-cd393a662778');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ahkhIR1Z5c9"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def scale_ts(vls):\n",
        "    sc = RobustScaler()\n",
        "    return sc.fit_transform(vls.reshape(-1,1)).ravel()\n",
        "def scale_sts(vls):\n",
        "    sc = StandardScaler()\n",
        "    return sc.fit_transform(vls.reshape(-1,1)).ravel()\n",
        "def scale_time_ts(vls):\n",
        "    sc = MinMaxScaler()\n",
        "    return sc.fit_transform(vls.reshape(-1,1)).ravel()\n",
        "\n",
        "class CardioDataset(Dataset):\n",
        "    def __init__(self, df, win_size=33):\n",
        "        self.df = df.sort_values(['id','time']).reset_index(drop=True).copy()\n",
        "        self.df['time'] = df.groupby('id')['time'].agg('diff').fillna(0).values\n",
        "        self.df['time'] = scale_time_ts(self.df['time'].values)\n",
        "        self.df['x'] = scale_ts(self.df['x'].values)\n",
        "        self.win_size = win_size\n",
        "\n",
        "        self.start_index = []\n",
        "        self.end_index = []\n",
        "\n",
        "        total_len = 0\n",
        "        for q,qdf in self.df.groupby('id'):\n",
        "            #qdf['x'] = scale_ts(qdf['x'].values)\n",
        "            for i in range(qdf.shape[0]):\n",
        "                self.start_index.append(max(total_len, total_len + i - win_size // 2))\n",
        "                self.end_index.append(min(total_len + qdf.shape[0], total_len + i + win_size // 2))\n",
        "            total_len += qdf.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i0 = self.start_index[idx]\n",
        "        i1 = self.end_index[idx]\n",
        "\n",
        "        x_mat = np.zeros((self.win_size,1))\n",
        "        s = i1 - i0\n",
        "        x_mat[-s:,0] = scale_ts(self.df.iloc[i0:i1].x.values)\n",
        "        #x_mat[-s:,0] = self.df.iloc[i0:i1].x.values\n",
        "        \n",
        "        return {\"x\": x_mat,\n",
        "                \"y\": [self.df.y.values[idx]],\n",
        "                \"start\": i0,\n",
        "                \"end\": i1\n",
        "               }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsEg7OS_fzsV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vACws6kfmzlF"
      },
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n",
        "\n",
        "def threshold_search(y_true, y_proba):\n",
        "    precision , recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
        "    thresholds = np.append(thresholds, 1.001) \n",
        "    F = 2 / (1/precision + 1/recall)\n",
        "    best_score = np.max(F)\n",
        "    best_th = thresholds[np.argmax(F)]\n",
        "    return best_th , best_score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep0hOJh5o1Ph",
        "outputId": "51071b40-471f-4eb9-960f-6539a459199f"
      },
      "source": [
        "%%writefile lookahead.py\n",
        "# Lookahead implementation from https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/lookahead.py\n",
        "\n",
        "\"\"\" Lookahead Optimizer Wrapper.\n",
        "Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
        "Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from collections import defaultdict\n",
        "\n",
        "class Lookahead(Optimizer):\n",
        "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
        "        self.base_optimizer = base_optimizer\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults = base_optimizer.defaults\n",
        "        self.defaults.update(defaults)\n",
        "        self.state = defaultdict(dict)\n",
        "        # manually add our defaults to the param groups\n",
        "        for name, default in defaults.items():\n",
        "            for group in self.param_groups:\n",
        "                group.setdefault(name, default)\n",
        "\n",
        "    def update_slow(self, group):\n",
        "        for fast_p in group[\"params\"]:\n",
        "            if fast_p.grad is None:\n",
        "                continue\n",
        "            param_state = self.state[fast_p]\n",
        "            if 'slow_buffer' not in param_state:\n",
        "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
        "                param_state['slow_buffer'].copy_(fast_p.data)\n",
        "            slow = param_state['slow_buffer']\n",
        "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
        "            fast_p.data.copy_(slow)\n",
        "\n",
        "    def sync_lookahead(self):\n",
        "        for group in self.param_groups:\n",
        "            self.update_slow(group)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # print(self.k)\n",
        "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
        "        loss = self.base_optimizer.step(closure)\n",
        "        for group in self.param_groups:\n",
        "            group['lookahead_step'] += 1\n",
        "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
        "                self.update_slow(group)\n",
        "        return loss\n",
        "\n",
        "    def state_dict(self):\n",
        "        fast_state_dict = self.base_optimizer.state_dict()\n",
        "        slow_state = {\n",
        "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
        "            for k, v in self.state.items()\n",
        "        }\n",
        "        fast_state = fast_state_dict['state']\n",
        "        param_groups = fast_state_dict['param_groups']\n",
        "        return {\n",
        "            'state': fast_state,\n",
        "            'slow_state': slow_state,\n",
        "            'param_groups': param_groups,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        fast_state_dict = {\n",
        "            'state': state_dict['state'],\n",
        "            'param_groups': state_dict['param_groups'],\n",
        "        }\n",
        "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
        "\n",
        "        # We want to restore the slow state, but share param_groups reference\n",
        "        # with base_optimizer. This is a bit redundant but least code\n",
        "        slow_state_new = False\n",
        "        if 'slow_state' not in state_dict:\n",
        "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
        "            state_dict['slow_state'] = defaultdict(dict)\n",
        "            slow_state_new = True\n",
        "        slow_state_dict = {\n",
        "            'state': state_dict['slow_state'],\n",
        "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
        "        }\n",
        "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
        "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
        "        if slow_state_new:\n",
        "            # reapply defaults to catch missing lookahead specific ones\n",
        "            for name, default in self.defaults.items():\n",
        "                for group in self.param_groups:\n",
        "                    group.setdefault(name, default)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lookahead.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvnXUf0To6Lu",
        "outputId": "2935cdc4-6e42-4a4f-820a-b890f1516812"
      },
      "source": [
        "%%writefile ralamb.py\n",
        "import torch, math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "# RAdam + LARS\n",
        "class Ralamb(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(Ralamb, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Ralamb, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # m_t\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # v_t\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = self.buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = radam_step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                radam_step = p_data_fp32.clone()\n",
        "                if N_sma >= 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
        "\n",
        "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
        "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
        "                if weight_norm == 0 or radam_norm == 0:\n",
        "                    trust_ratio = 1\n",
        "                else:\n",
        "                    trust_ratio = weight_norm / radam_norm\n",
        "\n",
        "                state['weight_norm'] = weight_norm\n",
        "                state['adam_norm'] = radam_norm\n",
        "                state['trust_ratio'] = trust_ratio\n",
        "\n",
        "                if N_sma >= 5:\n",
        "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting ralamb.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkeKVWauo8az"
      },
      "source": [
        "from lookahead import *\n",
        "from ralamb import * \n",
        "\n",
        "def Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n",
        "     ralamb = Ralamb(params, *args, **kwargs)\n",
        "     return Lookahead(ralamb, alpha, k)\n",
        "\n",
        "RangerLars = Over9000"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNHDupvdxJJ6"
      },
      "source": [
        "class CardioCnn1(nn.Module):\n",
        "    def __init__(self, win_size, output_size = 1, \n",
        "                 out_chan = 32, \n",
        "                 top_classifier_units = 16,\n",
        "                 rnn_units = 16,\n",
        "                 kern_sizes = [9,5,7,3]):\n",
        "        super().__init__()\n",
        "        self.cnn0 = nn.Conv1d(1, out_chan, kern_sizes[0])\n",
        "        self.cnn1 = nn.Conv1d(32, out_chan, kern_sizes[1])\n",
        "        self.cnn2 = nn.Conv1d(32, out_chan, kern_sizes[2])\n",
        "        self.cnn3 = nn.Conv1d(32, out_chan, kern_sizes[3])\n",
        "\n",
        "        self._gru = nn.GRU(input_size=1, \n",
        "                           num_layers=4,\n",
        "                           dropout=0.3,\n",
        "                           hidden_size=rnn_units, \n",
        "                           batch_first=True, \n",
        "                           bidirectional=True)\n",
        "\n",
        "        self.sum_chans = 37\n",
        "\n",
        "        self._head = nn.Sequential(\n",
        "            nn.LayerNorm(self.sum_chans),\n",
        "            nn.Linear(self.sum_chans, top_classifier_units),\n",
        "            nn.LayerNorm(top_classifier_units),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(top_classifier_units, top_classifier_units),\n",
        "            nn.LayerNorm(top_classifier_units),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(top_classifier_units, output_size),            \n",
        "        ) \n",
        "        \n",
        "    def forward(self, x_feats):\n",
        "        encoded,_ = self._gru(x_feats)\n",
        "        encoded0 = self.cnn0(x_feats.swapaxes(1,2))\n",
        "        encoded1 = self.cnn1(encoded0)\n",
        "        encoded2 = self.cnn2(encoded1)\n",
        "        encoded3 = self.cnn3(encoded2)\n",
        "\n",
        "        x6 = torch.max(encoded3, axis=1).values\n",
        "        x6 = torch.squeeze(x6)\n",
        "\n",
        "        x8 = torch.max(encoded, axis=1).values\n",
        "        x8 = torch.squeeze(x8)\n",
        "\n",
        "        x = torch.cat([x6,x8],-1)\n",
        "        x = self._head(x)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onIZtfPTDrDv"
      },
      "source": [
        "class CardioCnn(nn.Module):\n",
        "    def __init__(self, win_size, output_size = 1, \n",
        "                 out_chan = 32, \n",
        "                 top_classifier_units = 32,\n",
        "                 rnn_units = 8,\n",
        "                 kern_sizes = [3,5,7,9]):\n",
        "        super().__init__()\n",
        "        self.cnn0 = nn.Conv1d(1, out_chan, kern_sizes[0])\n",
        "        self.cnn1 = nn.Conv1d(1, out_chan, kern_sizes[1])\n",
        "        self.cnn2 = nn.Conv1d(1, out_chan, kern_sizes[2])\n",
        "        self.cnn3 = nn.Conv1d(1, out_chan, kern_sizes[3])\n",
        "\n",
        "        self._gru = nn.GRU(input_size=1, \n",
        "                           num_layers=4,\n",
        "                           dropout=0.3,\n",
        "                           hidden_size=rnn_units, \n",
        "                           batch_first=True, \n",
        "                           bidirectional=True)\n",
        "\n",
        "        self.sum_chans = (win_size*4 - np.sum(kern_sizes) + 4) + 2*rnn_units\n",
        "\n",
        "        self._head = nn.Sequential(\n",
        "            nn.LayerNorm(self.sum_chans),\n",
        "            nn.Linear(self.sum_chans, top_classifier_units),\n",
        "            nn.LayerNorm(top_classifier_units),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(top_classifier_units, top_classifier_units),\n",
        "            nn.LayerNorm(top_classifier_units),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(top_classifier_units, output_size),            \n",
        "        ) \n",
        "        \n",
        "    def forward(self, x_feats):\n",
        "        encoded,_ = self._gru(x_feats)\n",
        "        encoded0 = self.cnn0(x_feats.swapaxes(1,2))\n",
        "        encoded1 = self.cnn1(x_feats.swapaxes(1,2))\n",
        "        encoded2 = self.cnn2(x_feats.swapaxes(1,2))\n",
        "        encoded3 = self.cnn3(x_feats.swapaxes(1,2))\n",
        "\n",
        "        x0 = torch.max(encoded0, axis=1).values\n",
        "        x0 = torch.squeeze(x0)\n",
        "        x1 = torch.mean(encoded0, axis=1)\n",
        "        x1 = torch.squeeze(x1)\n",
        "\n",
        "        x2 = torch.max(encoded1, axis=1).values\n",
        "        x2 = torch.squeeze(x2)\n",
        "        x3 = torch.mean(encoded1, axis=1)\n",
        "        x3 = torch.squeeze(x3)\n",
        "\n",
        "        x4 = torch.max(encoded2, axis=1).values\n",
        "        x4 = torch.squeeze(x4)\n",
        "        x5 = torch.mean(encoded2, axis=1)\n",
        "        x5 = torch.squeeze(x5)\n",
        "\n",
        "        x6 = torch.max(encoded3, axis=1).values\n",
        "        x6 = torch.squeeze(x6)\n",
        "        x7 = torch.mean(encoded3, axis=1)\n",
        "        x7 = torch.squeeze(x7)\n",
        "\n",
        "        x8 = torch.max(encoded, axis=1).values\n",
        "        x8 = torch.squeeze(x8)\n",
        "\n",
        "        x = torch.cat([x0,x2,x4,x6,x8],-1)\n",
        "        x = self._head(x)\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQrNU87sEKT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f37aab-39a4-4c70-89df-6fa78d04287f"
      },
      "source": [
        "import joblib \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import RandomSampler, SequentialSampler, DataLoader\n",
        "import tqdm\n",
        "\n",
        "WIN_SIZE = 17\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "ifold = 0\n",
        "fold_metrics = []\n",
        "for tr_id, va_id in kf.split(df['id'].unique()):\n",
        "    train_df = df.loc[df['id'].isin(tr_id)].reset_index(drop=True)\n",
        "    valid_df = df.loc[df['id'].isin(va_id)].reset_index(drop=True)\n",
        "\n",
        "    train_ds = CardioDataset(train_df, WIN_SIZE)\n",
        "    valid_ds = CardioDataset(valid_df, WIN_SIZE)\n",
        "\n",
        "    train_sampler = RandomSampler(train_ds)\n",
        "    valid_sampler = SequentialSampler(valid_ds)\n",
        "    batch_size = 1024\n",
        "    train_dl = DataLoader(train_ds, sampler=train_sampler, batch_size=batch_size, num_workers=2)\n",
        "    valid_dl = DataLoader(valid_ds, sampler=valid_sampler, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    model = CardioCnn(WIN_SIZE, 1).to(device)\n",
        "\n",
        "    nepochs = 30\n",
        "\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                          lr = 1e-3                          \n",
        "                       )\n",
        "    \n",
        "    optimizer = Lookahead(optimizer, 0.5, 6)\n",
        "    '''\n",
        "    optimizer = RangerLars(model.parameters(), lr=1e-3)\n",
        "    '''\n",
        "    for epoch in range(nepochs):\n",
        "        model.train();\n",
        "        optimizer.zero_grad()\n",
        "        i = 0\n",
        "        train_losses = []\n",
        "        for x in train_dl:\n",
        "            out = model(x['x'].float().to(device))\n",
        "            loss = criterion(out.flatten(), x['y'][0].float().to(device)) \n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "            optimizer.zero_grad()\n",
        "            i += 1\n",
        "        #torch.save(model.state_dict(), 'model_'+str(epoch) + '.pth')\n",
        "            train_losses.append(loss.item())\n",
        "        \n",
        "        valid_losses = []\n",
        "        valid_df_predict = np.zeros(valid_df.shape[0])\n",
        "        model.eval();\n",
        "        total_len = 0\n",
        "        for xx in valid_dl:\n",
        "            out = model(xx['x'].float().to(device))\n",
        "            valid_loss = criterion(out.flatten(), xx['y'][0].float().to(device)) \n",
        "            valid_losses.append(valid_loss.item())\n",
        "            out = torch.sigmoid(out.flatten()).detach().cpu().numpy()\n",
        "            for j in range(xx['x'].shape[0]):\n",
        "                i0 = total_len + j\n",
        "                valid_df_predict[i0] = out[j]\n",
        "            total_len += xx['x'].shape[0]\n",
        "        y_proba = valid_df_predict\n",
        "        y_true = valid_df.y.values\n",
        "\n",
        "        best_th , best_score = threshold_search(y_true, y_proba)\n",
        "        #best_th = 0.066\n",
        "        #best_score = f1_score(y_true.astype(int), (y_proba > best_th).astype(int)) #, average='micro'\n",
        "        print('fold', ifold, 'epoch', epoch, 'best_th', best_th, 'f1-score', best_score) #'step', i,\n",
        "        print('fold', ifold, 'epoch', epoch, 'train_loss', np.mean(train_losses), 'valid_loss', np.mean(valid_losses)) \n",
        "        model.train();\n",
        "\n",
        "    fold_metrics.append(best_score)\n",
        "    ifold += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 0 epoch 0 best_th 0.15665209293365479 f1-score 0.327217125382263\n",
            "fold 0 epoch 0 train_loss 0.47142459026404787 valid_loss 0.40159839527173474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 0 epoch 1 best_th 0.11991103738546371 f1-score 0.35934472432622866\n",
            "fold 0 epoch 1 train_loss 0.4051912519193831 valid_loss 0.3917012092742053\n",
            "fold 0 epoch 2 best_th 0.12936337292194366 f1-score 0.394963022186688\n",
            "fold 0 epoch 2 train_loss 0.3834331645852044 valid_loss 0.36691246926784515\n",
            "fold 0 epoch 3 best_th 0.29211464524269104 f1-score 0.610727969348659\n",
            "fold 0 epoch 3 train_loss 0.3420410801966985 valid_loss 0.2908031926913695\n",
            "fold 0 epoch 4 best_th 0.14546628296375275 f1-score 0.7156611039794609\n",
            "fold 0 epoch 4 train_loss 0.2721460479355994 valid_loss 0.23315543613650583\n",
            "fold 0 epoch 5 best_th 0.15395276248455048 f1-score 0.7381103360811667\n",
            "fold 0 epoch 5 train_loss 0.2321420003260885 valid_loss 0.21233875168995422\n",
            "fold 0 epoch 6 best_th 0.14171679317951202 f1-score 0.7502423263327949\n",
            "fold 0 epoch 6 train_loss 0.21412794895115353 valid_loss 0.2077850733291019\n",
            "fold 0 epoch 7 best_th 0.1292906254529953 f1-score 0.7557692307692309\n",
            "fold 0 epoch 7 train_loss 0.20512528078896658 valid_loss 0.2046842019666325\n",
            "fold 0 epoch 8 best_th 0.17651961743831635 f1-score 0.7611075338055376\n",
            "fold 0 epoch 8 train_loss 0.20244070497297106 valid_loss 0.19571460851214148\n",
            "fold 0 epoch 9 best_th 0.1808844655752182 f1-score 0.7715868361029651\n",
            "fold 0 epoch 9 train_loss 0.19355386708463942 valid_loss 0.1950214315544475\n",
            "fold 0 epoch 10 best_th 0.19868046045303345 f1-score 0.7710687762350662\n",
            "fold 0 epoch 10 train_loss 0.19027455505870638 valid_loss 0.19008290835402228\n",
            "fold 0 epoch 11 best_th 0.21022048592567444 f1-score 0.7777418311226141\n",
            "fold 0 epoch 11 train_loss 0.1883073885525976 valid_loss 0.1853440762920813\n",
            "fold 0 epoch 12 best_th 0.2021128535270691 f1-score 0.7802056555269923\n",
            "fold 0 epoch 12 train_loss 0.18321432581260091 valid_loss 0.18371993777426807\n",
            "fold 0 epoch 13 best_th 0.30881014466285706 f1-score 0.7769640479360853\n",
            "fold 0 epoch 13 train_loss 0.183656612322444 valid_loss 0.1819386827674779\n",
            "fold 0 epoch 14 best_th 0.24810081720352173 f1-score 0.7725376965030477\n",
            "fold 0 epoch 14 train_loss 0.18137605204468682 valid_loss 0.18385222825137051\n",
            "fold 0 epoch 15 best_th 0.32464170455932617 f1-score 0.7808951235804943\n",
            "fold 0 epoch 15 train_loss 0.17801509868531 valid_loss 0.17889739437536759\n",
            "fold 0 epoch 16 best_th 0.23288637399673462 f1-score 0.7809096732863549\n",
            "fold 0 epoch 16 train_loss 0.17559580079146794 valid_loss 0.17838388816876846\n",
            "fold 0 epoch 17 best_th 0.48042282462120056 f1-score 0.7773224043715846\n",
            "fold 0 epoch 17 train_loss 0.17559984113488877 valid_loss 0.1790919452905655\n",
            "fold 0 epoch 18 best_th 0.4452701807022095 f1-score 0.780437756497948\n",
            "fold 0 epoch 18 train_loss 0.17210875345127924 valid_loss 0.17785665257410568\n",
            "fold 0 epoch 19 best_th 0.4413200616836548 f1-score 0.7833163784333671\n",
            "fold 0 epoch 19 train_loss 0.17038378438779286 valid_loss 0.17451957206834445\n",
            "fold 0 epoch 20 best_th 0.41076722741127014 f1-score 0.7845681119836121\n",
            "fold 0 epoch 20 train_loss 0.1705377446044059 valid_loss 0.17663251947272907\n",
            "fold 0 epoch 21 best_th 0.4495636522769928 f1-score 0.7885597548518897\n",
            "fold 0 epoch 21 train_loss 0.1678089192580609 valid_loss 0.17360062829472803\n",
            "fold 0 epoch 22 best_th 0.4540501832962036 f1-score 0.7842069434989789\n",
            "fold 0 epoch 22 train_loss 0.166998538232985 valid_loss 0.17535425180738623\n",
            "fold 0 epoch 23 best_th 0.4502095580101013 f1-score 0.7903989181879648\n",
            "fold 0 epoch 23 train_loss 0.16609866739738555 valid_loss 0.17196641930124976\n",
            "fold 0 epoch 24 best_th 0.2864345908164978 f1-score 0.7884114583333334\n",
            "fold 0 epoch 24 train_loss 0.16641320146265484 valid_loss 0.17617841539057819\n",
            "fold 0 epoch 25 best_th 0.4692564308643341 f1-score 0.7875894988066825\n",
            "fold 0 epoch 25 train_loss 0.16294840616839273 valid_loss 0.17592279680750586\n",
            "fold 0 epoch 26 best_th 0.43732723593711853 f1-score 0.7893155258764607\n",
            "fold 0 epoch 26 train_loss 0.16217358765147982 valid_loss 0.1730061173439026\n",
            "fold 0 epoch 27 best_th 0.33770883083343506 f1-score 0.7902855267476205\n",
            "fold 0 epoch 27 train_loss 0.16122391110374815 valid_loss 0.1743475855751471\n",
            "fold 0 epoch 28 best_th 0.2785850167274475 f1-score 0.7905927835051546\n",
            "fold 0 epoch 28 train_loss 0.15907403046176546 valid_loss 0.17261290143836627\n",
            "fold 0 epoch 29 best_th 0.3486904203891754 f1-score 0.7924650860669048\n",
            "fold 0 epoch 29 train_loss 0.1563903315081483 valid_loss 0.17493680254979568\n",
            "fold 1 epoch 0 best_th 0.15365514159202576 f1-score 0.31073162564875273\n",
            "fold 1 epoch 0 train_loss 0.45653808881074953 valid_loss 0.3463642948440143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 1 epoch 1 best_th 0.16994647681713104 f1-score 0.36209751609935603\n",
            "fold 1 epoch 1 train_loss 0.4246168419336661 valid_loss 0.3271635557923998\n",
            "fold 1 epoch 2 best_th 0.35895806550979614 f1-score 0.4596476853748464\n",
            "fold 1 epoch 2 train_loss 0.3876115305301471 valid_loss 0.29312003191028324\n",
            "fold 1 epoch 3 best_th 0.4370797574520111 f1-score 0.5977742448330684\n",
            "fold 1 epoch 3 train_loss 0.33790309001237917 valid_loss 0.2492177731224469\n",
            "fold 1 epoch 4 best_th 0.6244283318519592 f1-score 0.6342857142857143\n",
            "fold 1 epoch 4 train_loss 0.2703602684613986 valid_loss 0.22408693337014743\n",
            "fold 1 epoch 5 best_th 0.6337279081344604 f1-score 0.6414740787008121\n",
            "fold 1 epoch 5 train_loss 0.23794980423572737 valid_loss 0.22317922062107495\n",
            "fold 1 epoch 6 best_th 0.6553953886032104 f1-score 0.6465976795233616\n",
            "fold 1 epoch 6 train_loss 0.20843807894449967 valid_loss 0.22594456600823573\n",
            "fold 1 epoch 7 best_th 0.5878229141235352 f1-score 0.6434250764525994\n",
            "fold 1 epoch 7 train_loss 0.19927828873579317 valid_loss 0.23033865168690681\n",
            "fold 1 epoch 8 best_th 0.6463984847068787 f1-score 0.6453186947240013\n",
            "fold 1 epoch 8 train_loss 0.19146442824067214 valid_loss 0.23590589234871523\n",
            "fold 1 epoch 9 best_th 0.6790413856506348 f1-score 0.6484326982175783\n",
            "fold 1 epoch 9 train_loss 0.19741546649199265 valid_loss 0.23534046964985983\n",
            "fold 1 epoch 10 best_th 0.556469738483429 f1-score 0.6493663246831625\n",
            "fold 1 epoch 10 train_loss 0.19086673932197767 valid_loss 0.231592720374465\n",
            "fold 1 epoch 11 best_th 0.6065698266029358 f1-score 0.6495828367103695\n",
            "fold 1 epoch 11 train_loss 0.18736944022851112 valid_loss 0.23746911968503678\n",
            "fold 1 epoch 12 best_th 0.42251431941986084 f1-score 0.6536527654540077\n",
            "fold 1 epoch 12 train_loss 0.18555540954455352 valid_loss 0.22894946226317966\n",
            "fold 1 epoch 13 best_th 0.49844062328338623 f1-score 0.6530729465824239\n",
            "fold 1 epoch 13 train_loss 0.18868961242529061 valid_loss 0.2377147940652711\n",
            "fold 1 epoch 14 best_th 0.6090624332427979 f1-score 0.6547299313637719\n",
            "fold 1 epoch 14 train_loss 0.18788491533352777 valid_loss 0.23459013292033756\n",
            "fold 1 epoch 15 best_th 0.5875289440155029 f1-score 0.6551826551826552\n",
            "fold 1 epoch 15 train_loss 0.1764988769323398 valid_loss 0.23469793078090465\n",
            "fold 1 epoch 16 best_th 0.7318471670150757 f1-score 0.6599326599326599\n",
            "fold 1 epoch 16 train_loss 0.17980529635380477 valid_loss 0.24193553466882026\n",
            "fold 1 epoch 17 best_th 0.5890565514564514 f1-score 0.6577460600654178\n",
            "fold 1 epoch 17 train_loss 0.1854627686433303 valid_loss 0.23396969560001576\n",
            "fold 1 epoch 18 best_th 0.44256097078323364 f1-score 0.65748031496063\n",
            "fold 1 epoch 18 train_loss 0.18221218654742607 valid_loss 0.23040530544572643\n",
            "fold 1 epoch 19 best_th 0.6585202813148499 f1-score 0.6568174823022468\n",
            "fold 1 epoch 19 train_loss 0.17166372417257383 valid_loss 0.23195220224027122\n",
            "fold 1 epoch 20 best_th 0.6851336359977722 f1-score 0.6590978827861307\n",
            "fold 1 epoch 20 train_loss 0.16869849085998842 valid_loss 0.23498032255364315\n",
            "fold 1 epoch 21 best_th 0.6975400447845459 f1-score 0.6602544213465716\n",
            "fold 1 epoch 21 train_loss 0.18184340305817434 valid_loss 0.23119240533560514\n",
            "fold 1 epoch 22 best_th 0.6837729215621948 f1-score 0.6603716113310997\n",
            "fold 1 epoch 22 train_loss 0.16912473547152984 valid_loss 0.2328790710972888\n",
            "fold 1 epoch 23 best_th 0.6247133612632751 f1-score 0.6610118146016359\n",
            "fold 1 epoch 23 train_loss 0.16569897159934044 valid_loss 0.22876396096710647\n",
            "fold 1 epoch 24 best_th 0.6860201358795166 f1-score 0.6619588578446424\n",
            "fold 1 epoch 24 train_loss 0.17178132747992492 valid_loss 0.23210228101483413\n",
            "fold 1 epoch 25 best_th 0.6656057238578796 f1-score 0.6656525707331913\n",
            "fold 1 epoch 25 train_loss 0.17910534143447876 valid_loss 0.22788115604115383\n",
            "fold 1 epoch 26 best_th 0.7739207148551941 f1-score 0.6652173913043479\n",
            "fold 1 epoch 26 train_loss 0.17845493975358132 valid_loss 0.23926106467843056\n",
            "fold 1 epoch 27 best_th 0.6451491117477417 f1-score 0.6630532971996387\n",
            "fold 1 epoch 27 train_loss 0.168505733498396 valid_loss 0.22759563542370284\n",
            "fold 1 epoch 28 best_th 0.5767000913619995 f1-score 0.6629013079667062\n",
            "fold 1 epoch 28 train_loss 0.16882614772289228 valid_loss 0.22613314326320375\n",
            "fold 1 epoch 29 best_th 0.6735497713088989 f1-score 0.665024630541872\n",
            "fold 1 epoch 29 train_loss 0.16197070422080848 valid_loss 0.22857835795730352\n",
            "fold 2 epoch 0 best_th 0.1361045390367508 f1-score 0.300335901995653\n",
            "fold 2 epoch 0 train_loss 0.45767113044857977 valid_loss 0.34542333086331684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2 epoch 1 best_th 0.13210336863994598 f1-score 0.3330064718572269\n",
            "fold 2 epoch 1 train_loss 0.4164383813738823 valid_loss 0.3334173771242301\n",
            "fold 2 epoch 2 best_th 0.1374083161354065 f1-score 0.35626242544731607\n",
            "fold 2 epoch 2 train_loss 0.40397013202309606 valid_loss 0.3199323272953431\n",
            "fold 2 epoch 3 best_th 0.2821628451347351 f1-score 0.4460835967685282\n",
            "fold 2 epoch 3 train_loss 0.38476394265890124 valid_loss 0.28761005960404873\n",
            "fold 2 epoch 4 best_th 0.31215187907218933 f1-score 0.6971830985915494\n",
            "fold 2 epoch 4 train_loss 0.34140932112932204 valid_loss 0.2158253025263548\n",
            "fold 2 epoch 5 best_th 0.34277886152267456 f1-score 0.7633587786259541\n",
            "fold 2 epoch 5 train_loss 0.2738998580724001 valid_loss 0.1631833485638102\n",
            "fold 2 epoch 6 best_th 0.2162344753742218 f1-score 0.7782156932998926\n",
            "fold 2 epoch 6 train_loss 0.2379907626658678 valid_loss 0.15124413060645261\n",
            "fold 2 epoch 7 best_th 0.2389933168888092 f1-score 0.7847730600292826\n",
            "fold 2 epoch 7 train_loss 0.2230108942836523 valid_loss 0.14774675543109575\n",
            "fold 2 epoch 8 best_th 0.357112854719162 f1-score 0.7884761182714178\n",
            "fold 2 epoch 8 train_loss 0.2158587496727705 valid_loss 0.1441349977006515\n",
            "fold 2 epoch 9 best_th 0.40969881415367126 f1-score 0.7904146063141879\n",
            "fold 2 epoch 9 train_loss 0.21108304671943187 valid_loss 0.14076203977068266\n",
            "fold 2 epoch 10 best_th 0.4107593595981598 f1-score 0.7931547619047619\n",
            "fold 2 epoch 10 train_loss 0.20556006208062172 valid_loss 0.13782225425044695\n",
            "fold 2 epoch 11 best_th 0.34378665685653687 f1-score 0.7955473098330241\n",
            "fold 2 epoch 11 train_loss 0.20242618918418884 valid_loss 0.13914234843105078\n",
            "fold 2 epoch 12 best_th 0.3414158821105957 f1-score 0.7982359426681366\n",
            "fold 2 epoch 12 train_loss 0.19939429834485053 valid_loss 0.13555644607792297\n",
            "fold 2 epoch 13 best_th 0.3516969382762909 f1-score 0.7997038134024436\n",
            "fold 2 epoch 13 train_loss 0.19649481996893883 valid_loss 0.1357071747382482\n",
            "fold 2 epoch 14 best_th 0.40338844060897827 f1-score 0.8010471204188482\n",
            "fold 2 epoch 14 train_loss 0.19419989474117755 valid_loss 0.1371860078846415\n",
            "fold 2 epoch 15 best_th 0.3334605395793915 f1-score 0.8035255233198678\n",
            "fold 2 epoch 15 train_loss 0.1911568518728018 valid_loss 0.135847644880414\n",
            "fold 2 epoch 16 best_th 0.3713226616382599 f1-score 0.8029684601113172\n",
            "fold 2 epoch 16 train_loss 0.18938379399478436 valid_loss 0.13466895868380865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_zl3iAfd85Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
